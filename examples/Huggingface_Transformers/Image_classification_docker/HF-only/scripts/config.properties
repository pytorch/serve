'''
References:
https://github.com/pytorch/serve/blob/master/docs/configuration.md
'''

'''
The addresses can be changed to another port as described in the `configuration.md`
Keep in mind that the same should reflect in all the POST, PUT and DELETE requests for Management & Inference APIs

Check out this section if you want to include the functionality to send inference requests from another node/device to the Torchserver server device:
https://github.com/pytorch/serve/blob/master/docs/configuration.md#enable-ssl
'''
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
number_of_netty_threads=32
job_queue_size=1000


'''
The path of folder in which model archives (.mar files) are present
This can be overrided by `--model-store` option in the `torchserve --start --ts-config config.properties` command
Reference: https://github.com/pytorch/serve/blob/master/docs/configuration.md#command-line-parameters 

However, since the objective in this repo is to deploy Docker containers, we leave it as it is
'''
model_store=model-store


'''
Indicates that no model present in the `model_store` folder would be loaded during `torchserve --start` command
Available options: https://github.com/pytorch/serve/blob/master/docs/configuration.md#load-models-at-startup
'''
load_models="standalone"


'''

'''
default_workers_per_model=1


'''
This line is important for installing custom requirements in Torchserve Docker container with `requirements.txt` file, like `transformers` library
https://pytorch.org/serve/configuration.html#allow-model-specific-custom-python-packages
https://github.com/pytorch/serve/tree/master/model-archiver#torch-model-archiver-command-line-interface
'''

install_py_dep_per_model=true
