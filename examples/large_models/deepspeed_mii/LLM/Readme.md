# Running LLM model using Microsoft DeepSpeed-MII in Torchserve

This example demo serving HF LLM model with Microsoft DeepSpeed-MII in Torchserve. With DeepSpeed-MII there has been significant progress in system optimizations for DL model inference, drastically reducing both latency and cost.

The notebook example can be found in mii-deepspeed-fastgen.ipynb.
