# TorchServe frontend parameters
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 1200
parallelType: "tp"
deviceType: "gpu"
# example of user specified GPU deviceIds
deviceIds: [0,1,2,3] # seting CUDA_VISIBLE_DEVICES

torchrun:
    nproc-per-node: 4

# TorchServe Backend parameters
handler:
    model_name: "meta-llama/Llama-2-13b-hf"
    model_path: "model/models--meta-llama--Llama-2-13b-hf/snapshots/99afe33d7eaa87c7fc6ea2594a0e4e7e588ee0a4"
    tensor_parallel: 4
    max_length: 4096
    max_new_tokens: 256
