minWorkers: 1
maxWorkers: 1
maxBatchDelay: 0
batchSize: 8
responseTimeout: 10800
jobQueueSize: 500
continuousBatching: true

handler:
    model_path: "model/models--meta-llama--Llama-2-70b-hf/snapshots/90052941a64de02075ca800b09fcea1bdaacb939"
    model_checkpoint_dir: "llama-2-70b-split"
    model_module_prefix: "transformers_neuronx"
    model_class_name: "llama.model.LlamaForSampling"
    # see https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/transformers-neuronx/index.html#known-issues-and-limitations for llama2-13b
    # neuron_cc_flag: "-O1 --model-type=transformer --enable-mixed-precision-accumulation --enable-saturate-infinity"
    amp: "bf16"
    tp_degree: 24
    max_length: 256
    max_new_tokens: 50
    batch_size: 8
