minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 10800
batchSize: 16

handler:
    model_path: "model/models--meta-llama--Meta-Llama-3-70B/snapshots/b33784c5adf6e4b1a60d041da74e83fd438d67cd"
    model_checkpoint_dir: "llama-3-70b-split"
    model_module_prefix: "transformers_neuronx"
    model_class_name: "llama.model.LlamaForSampling"
    # see https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/transformers-neuronx/index.html#known-issues-and-limitations for llama2-13b
    # neuron_cc_flag: "-O1 --model-type=transformer --enable-mixed-precision-accumulation --enable-saturate-infinity"
    amp: "bf16"
    tp_degree: 24
    max_length: 256
    max_new_tokens: 50

micro_batching:
    micro_batch_size: 4
    parallelism:
        preprocess: 2
        inference: 1
        postprocess: 2
