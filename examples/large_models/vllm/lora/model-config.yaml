# TorchServe frontend parameters
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
startupTimeout: 1200
deviceType: "gpu"
asyncCommunication: true

handler:
    model_path: "model/models--meta-llama--Meta-Llama-3.1-8B/snapshots/48d6d0fc4e02fb1269b36940650a1b7233035cbb"
    vllm_engine_config:
        enable_lora: true
        max_loras: 4
        max_cpu_loras: 4
        max_lora_rank: 32
        max_num_seqs: 16
        max_model_len: 250
        served_model_name:
            - "meta-llama/Meta-Llama-3.1-8B"
            - "llama-8b-lora"

    adapters:
        adapter_1: "adapters/model/models--llama-duo--llama3.1-8b-summarize-gpt4o-128k/snapshots/4ba83353f24fa38946625c8cc49bf21c80a22825"
