


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Custom Service &mdash; PyTorch/Serve master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="TorchServe default inference handlers" href="default_handlers.html" />
    <link rel="prev" title="Advanced configuration" href="configuration.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master 
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">TorchServe</a></li>
<li class="toctree-l1"><a class="reference internal" href="Troubleshooting.html">Troubleshooting Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_guide.html">Performance Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="batch_inference_with_ts.html">Batch Inference with TorchServe</a></li>
<li class="toctree-l1"><a class="reference internal" href="code_coverage.html">Code Coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Advanced configuration</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Service</a></li>
<li class="toctree-l1"><a class="reference internal" href="default_handlers.html">TorchServe default inference handlers</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging in Torchserve</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">TorchServe Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="request_envelopes.html">Request Envelopes</a></li>
<li class="toctree-l1"><a class="reference internal" href="server.html">Running TorchServe</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">Running TorchServe with NVIDIA MPS</a></li>
<li class="toctree-l1"><a class="reference internal" href="snapshot.html">TorchServe model snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve_on_win_native.html">TorchServe on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve_on_wsl.html">TorchServe on Windows Subsystem for Linux (WSL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_cases.html">Torchserve Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflows.html">TorchServe Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="large_model_inference.html">Serving large models with Torchserve</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQs.html">FAQ’S</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Service APIs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="grpc_api.html">TorchServe gRPC API</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference_api.html">Inference API</a></li>
<li class="toctree-l1"><a class="reference internal" href="management_api.html">Management API</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics_api.html">Metrics API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest_api.html">TorchServe REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow_inference_api.html">Workflow Inference API</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow_management_api.html">Management API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer APIs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/ts.html">ts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/ts.torch_handler.request_envelope.html">ts.torch_handler.request_envelope package</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/ts.torch_handler.html">ts.torch_handler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/ts.metrics.html">ts.metrics package</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/ts.model_service.html">ts.model_service package</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/ts.protocol.html">ts.protocol package</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">serve</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="contents.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Custom Service</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/custom_service.md.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="custom-service">
<h1>Custom Service<a class="headerlink" href="#custom-service" title="Permalink to this heading">¶</a></h1>
<section id="contents-of-this-document">
<h2>Contents of this Document<a class="headerlink" href="#contents-of-this-document" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#custom-handlers">Custom handlers</a></p></li>
<li><p><a class="reference external" href="#creating-a-model-archive-with-an-entry-point">Creating model archive with entry point</a></p></li>
<li><p><a class="reference external" href="#handling-model-execution-on-multiple-gpus">Handling model execution on GPU</a></p></li>
<li><p><a class="reference external" href="#installing-model-specific-python-dependencies">Installing model specific python dependencies</a></p></li>
</ul>
</section>
<section id="custom-handlers">
<h2>Custom handlers<a class="headerlink" href="#custom-handlers" title="Permalink to this heading">¶</a></h2>
<p>Customize the behavior of TorchServe by writing a Python script that you package with
the model when you use the model archiver. TorchServe executes this code when it runs.</p>
<p>Provide a custom script to:</p>
<ul class="simple">
<li><p>Initialize the model instance</p></li>
<li><p>Pre-process input data before it is sent to the model for inference or Captum explanations</p></li>
<li><p>Customize how the model is invoked for inference or explanations</p></li>
<li><p>Post-process output from the model before sending back a response</p></li>
</ul>
<p>Following is applicable to all types of custom handlers</p>
<ul class="simple">
<li><p><strong>data</strong> - The input data from the incoming request</p></li>
<li><p><strong>context</strong> - Is the TorchServe <a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/context.py">context</a>. You can use following information for customization
model_name, model_dir, manifest, batch_size, gpu etc.</p></li>
</ul>
<section id="start-with-basehandler">
<h3>Start with BaseHandler!<a class="headerlink" href="#start-with-basehandler" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py">BaseHandler</a> implements most of the functionality you need. You can derive a new class from it, as shown in the examples and default handlers. Most of the time, you’ll only need to override <code class="docutils literal notranslate"><span class="pre">preprocess</span></code> or <code class="docutils literal notranslate"><span class="pre">postprocess</span></code>.</p>
</section>
<section id="custom-handler-with-module-level-entry-point">
<h3>Custom handler with <code class="docutils literal notranslate"><span class="pre">module</span></code> level entry point<a class="headerlink" href="#custom-handler-with-module-level-entry-point" title="Permalink to this heading">¶</a></h3>
<p>The custom handler file must define a module level function that acts as an entry point for execution.
The function can have any name, but it must accept the following parameters and return prediction results.</p>
<p>The signature of a entry point function is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create model object</span>
<span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">entry_point_function_name</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Works on data and context to create model object or process inference request.</span>
<span class="sd">    Following sample demonstrates how model object can be initialized for jit mode.</span>
<span class="sd">    Similarly you can do it for eager mode models.</span>
<span class="sd">    :param data: Input data for prediction</span>
<span class="sd">    :param context: context contains model server system properties</span>
<span class="sd">    :return: prediction output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">manifest</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">manifest</span>

        <span class="n">properties</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">system_properties</span>
        <span class="n">model_dir</span> <span class="o">=</span> <span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_dir&quot;</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gpu_id&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># Read model serialize/pt file</span>
        <span class="n">serialized_file</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">][</span><span class="s1">&#39;serializedFile&#39;</span><span class="p">]</span>
        <span class="n">model_pt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">serialized_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">model_pt_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Missing the model.pt file&quot;</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_pt_path</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1">#infer and return result</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>This entry point is engaged in two cases:</p>
<ol class="simple">
<li><p>TorchServe is asked to scale a model out to increase the number of backend workers (it is done either via a <code class="docutils literal notranslate"><span class="pre">PUT</span> <span class="pre">/models/{model_name}</span></code> request
or a <code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/models</span></code> request with <code class="docutils literal notranslate"><span class="pre">initial-workers</span></code> option or during TorchServe startup when you use the <code class="docutils literal notranslate"><span class="pre">--models</span></code> option (<code class="docutils literal notranslate"><span class="pre">torchserve</span> <span class="pre">--start</span> <span class="pre">--models</span> <span class="pre">{model_name=model.mar}</span></code>), ie., you provide model(s) to load)</p></li>
<li><p>TorchServe gets a <code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/predictions/{model_name}</span></code> request.</p></li>
</ol>
<p>(1) is used to scale-up or scale-down workers for a model. (2) is used as a standard way to run inference against a model. (1) is also known as model load time.
Typically, you want code for model initialization to run at model load time.
You can find out more about these and other TorchServe APIs in <a class="reference internal" href="management_api.html"><span class="doc">TorchServe Management API</span></a> and <a class="reference internal" href="inference_api.html"><span class="doc">TorchServe Inference API</span></a></p>
</section>
<section id="custom-handler-with-class-level-entry-point">
<h3>Custom handler with <code class="docutils literal notranslate"><span class="pre">class</span></code> level entry point<a class="headerlink" href="#custom-handler-with-class-level-entry-point" title="Permalink to this heading">¶</a></h3>
<p>You can create custom handler by having class with any name, but it must have an <code class="docutils literal notranslate"><span class="pre">initialize</span></code> and a <code class="docutils literal notranslate"><span class="pre">handle</span></code> method.</p>
<p>NOTE - If you plan to have multiple classes in same python module/file then make sure that handler class is the first in the list</p>
<p>The signature of a entry point class and functions is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ModelHandler</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom model handler implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Invoke by torchserve for loading a model</span>
<span class="sd">        :param context: context contains model server system properties</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1">#  load the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">manifest</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">manifest</span>

        <span class="n">properties</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">system_properties</span>
        <span class="n">model_dir</span> <span class="o">=</span> <span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_dir&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gpu_id&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># Read model serialize/pt file</span>
        <span class="n">serialized_file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">][</span><span class="s1">&#39;serializedFile&#39;</span><span class="p">]</span>
        <span class="n">model_pt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">serialized_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">model_pt_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Missing the model.pt file&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_pt_path</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">True</span>


    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Invoke by TorchServe for prediction request.</span>
<span class="sd">        Do pre-processing of data, prediction using model and postprocessing of prediciton output</span>
<span class="sd">        :param data: Input data for prediction</span>
<span class="sd">        :param context: Initial context contains model server system properties.</span>
<span class="sd">        :return: prediction output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pred_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_out</span>
</pre></div>
</div>
</section>
<section id="advanced-custom-handlers">
<h3>Advanced custom handlers<a class="headerlink" href="#advanced-custom-handlers" title="Permalink to this heading">¶</a></h3>
<section id="returning-custom-error-codes">
<h4>Returning custom error codes<a class="headerlink" href="#returning-custom-error-codes" title="Permalink to this heading">¶</a></h4>
<p>To return a custom error code back to the user via custom handler with <code class="docutils literal notranslate"><span class="pre">module</span></code> level entry point.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ts.utils.util</span> <span class="kn">import</span> <span class="n">PredictionException</span>
<span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="c1"># Some unexpected error - returning error code 513</span>
    <span class="k">raise</span> <span class="n">PredictionException</span><span class="p">(</span><span class="s2">&quot;Some Prediction Error&quot;</span><span class="p">,</span> <span class="mi">513</span><span class="p">)</span>
</pre></div>
</div>
<p>To return a custom error code back to the user via custom handler with <code class="docutils literal notranslate"><span class="pre">class</span></code> level entry point.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ts.torch_handler.base_handler</span> <span class="kn">import</span> <span class="n">BaseHandler</span>
<span class="kn">from</span> <span class="nn">ts.utils.util</span> <span class="kn">import</span> <span class="n">PredictionException</span>

<span class="k">class</span> <span class="nc">ModelHandler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom model handler implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="c1"># Some unexpected error - returning error code 513</span>
        <span class="k">raise</span> <span class="n">PredictionException</span><span class="p">(</span><span class="s2">&quot;Some Prediction Error&quot;</span><span class="p">,</span> <span class="mi">513</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="writing-a-custom-handler-from-scratch-for-prediction-and-explanations-request">
<h4>Writing a custom handler from scratch for Prediction and Explanations Request<a class="headerlink" href="#writing-a-custom-handler-from-scratch-for-prediction-and-explanations-request" title="Permalink to this heading">¶</a></h4>
<p><em>You should generally derive from BaseHandler and ONLY override methods whose behavior needs to change!</em> As you can see in the examples, most of the time you only need to override <code class="docutils literal notranslate"><span class="pre">preprocess</span></code> or <code class="docutils literal notranslate"><span class="pre">postprocess</span></code></p>
<p>Nonetheless, you are able to write a class from scratch. Below is an example. Basically, it follows a typical Init-Pre-Infer-Post pattern to create maintainable custom handler.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># custom handler file</span>

<span class="c1"># model_handler.py</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">ModelHandler defines a custom model handler.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">ts.torch_handler.base_handler</span> <span class="kn">import</span> <span class="n">BaseHandler</span>

<span class="k">class</span> <span class="nc">ModelHandler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom model handler implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explain</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize model. This will be called during model loading time</span>
<span class="sd">        :param context: Initial context contains model server system properties.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="n">context</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1">#  load the model, refer &#39;custom handler class&#39; above for details</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform raw input into model input data.</span>
<span class="sd">        :param batch: list of raw requests, should match batch size</span>
<span class="sd">        :return: list of preprocessed model input data</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Take the input data and make it inference ready</span>
        <span class="n">preprocessed_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">preprocessed_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">preprocessed_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;body&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">preprocessed_data</span>


    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_input</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal inference methods</span>
<span class="sd">        :param model_input: transformed model input data</span>
<span class="sd">        :return: list of inference output in NDArray</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Do some inference call to engine here and return output</span>
        <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model_output</span>

    <span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return inference result.</span>
<span class="sd">        :param inference_output: list of inference output</span>
<span class="sd">        :return: list of predict results</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Take output from network and post-process to desired format</span>
        <span class="n">postprocess_output</span> <span class="o">=</span> <span class="n">inference_output</span>
        <span class="k">return</span> <span class="n">postprocess_output</span>

    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Invoke by TorchServe for prediction request.</span>
<span class="sd">        Do pre-processing of data, prediction using model and postprocessing of prediciton output</span>
<span class="sd">        :param data: Input data for prediction</span>
<span class="sd">        :param context: Initial context contains model server system properties.</span>
<span class="sd">        :return: prediction output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</pre></div>
</div>
<p>Refer <a class="reference external" href="https://github.com/pytorch/serve/blob/master/examples/text_to_speech_synthesizer/waveglow_handler.py">waveglow_handler</a> for more details.</p>
</section>
<section id="captum-explanations-for-custom-handler">
<h4>Captum explanations for custom handler<a class="headerlink" href="#captum-explanations-for-custom-handler" title="Permalink to this heading">¶</a></h4>
<p>Torchserve returns the captum explanations for Image Classification, Text Classification and BERT models. It is achieved by placing the below request:
<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/explanations/{model_name}</span></code></p>
<p>The explanations are written as a part of the explain_handle method of base handler. The base handler invokes this explain_handle_method. The arguments that are passed to the explain handle methods are the pre-processed data and the raw data. It invokes the get insights function of the custom handler that returns the captum attributions. The user should write his own get_insights functionality to get the explanations</p>
<p>For serving a custom handler the captum algorithm should be initialized in the initialize functions of the handler</p>
<p>The user can override the explain_handle function in the custom handler.
The user should define their get_insights method for custom handler to get Captum Attributions.</p>
<p>The above ModelHandler class should have the following methods with captum functionality.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load the model and its artifacts</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">.....</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lig</span> <span class="o">=</span> <span class="n">LayerIntegratedGradients</span><span class="p">(</span>
                <span class="n">captum_sequence_forward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Invoke by TorchServe for prediction/explanation request.</span>
<span class="sd">        Do pre-processing of data, prediction using model and postprocessing of prediction/explanations output</span>
<span class="sd">        :param data: Input data for prediction/explanation</span>
<span class="sd">        :param context: Initial context contains model server system properties.</span>
<span class="sd">        :return: prediction/ explanations output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_explain</span><span class="p">():</span>
                <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
                <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explain_handle</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model_output</span>
    
    <span class="c1"># Present in the base_handler, so override only when neccessary</span>
    <span class="k">def</span> <span class="nf">explain_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_preprocess</span><span class="p">,</span> <span class="n">raw_data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Captum explanations handler</span>

<span class="sd">        Args:</span>
<span class="sd">            data_preprocess (Torch Tensor): Preprocessed data to be used for captum</span>
<span class="sd">            raw_data (list): The unprocessed data to get target from the request</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict : A dictionary response with the explanations response.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_explain</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">target</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Calculating Explanations&quot;</span><span class="p">)</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">raw_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Getting data and target&quot;</span><span class="p">)</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">row</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;body&quot;</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;target&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">target</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">output_explain</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_insights</span><span class="p">(</span><span class="n">data_preprocess</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_explain</span>

    <span class="k">def</span> <span class="nf">get_insights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Functionality to get the explanations.</span>
<span class="sd">        Called from the explain_handle method </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>
</pre></div>
</div>
</section>
<section id="extend-default-handlers">
<h4>Extend default handlers<a class="headerlink" href="#extend-default-handlers" title="Permalink to this heading">¶</a></h4>
<p>TorchServe has following default handlers.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/image_classifier.py">image_classifier</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/image_segmenter.py">image_segmenter</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/object_detector.py">object_detector</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/text_classifier.py">text_classifier</a></p></li>
</ul>
<p>If required above handlers can be extended to create custom handler. Also, you can extend abstract <a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py">base_handler</a>.</p>
<p>To import the default handler in a python script use the following import statement.</p>
<p><code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">ts.torch_handler.&lt;default_handler_name&gt;</span> <span class="pre">import</span> <span class="pre">&lt;DefaultHandlerClass&gt;</span></code></p>
<p>Following is an example of custom handler extending default image_classifier handler.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ts.torch_handler.image_classifier</span> <span class="kn">import</span> <span class="n">ImageClassifier</span>

<span class="k">class</span> <span class="nc">CustomImageClassifier</span><span class="p">(</span><span class="n">ImageClassifier</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overriding this method for custom preprocessing.</span>
<span class="sd">        :param data: raw data to be transformed</span>
<span class="sd">        :return: preprocessed data for model input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># custom pre-procsess code goes here</span>
        <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
<p>For more details refer following examples :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/serve/tree/master/examples/image_classifier">mnist digit classifier handler</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/examples/Huggingface_Transformers/Transformer_handler_generalized.py">Huggingface transformer generalized handler</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/serve/blob/master/examples/text_to_speech_synthesizer/waveglow_handler.py">Waveglow text to speech synthesizer</a></p></li>
</ul>
</section>
</section>
</section>
<section id="creating-a-model-archive-with-an-entry-point">
<h2>Creating a model archive with an entry point<a class="headerlink" href="#creating-a-model-archive-with-an-entry-point" title="Permalink to this heading">¶</a></h2>
<p>TorchServe identifies the entry point to the custom service from a manifest file.
When you create the model archive, specify the location of the entry point by using the <code class="docutils literal notranslate"><span class="pre">--handler</span></code> option.</p>
<p>The <a class="reference external" href="https://github.com/pytorch/serve/tree/master/model-archiver">model-archiver</a> tool enables you to create a model archive that TorchServe can serve.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torch-model-archiver<span class="w"> </span>--model-name<span class="w"> </span>&lt;model-name&gt;<span class="w"> </span>--version<span class="w"> </span>&lt;model_version_number&gt;<span class="w"> </span>--handler<span class="w"> </span>model_handler<span class="o">[</span>:&lt;entry_point_function_name&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>--model-file<span class="w"> </span>&lt;path_to_model_architecture_file&gt;<span class="o">]</span><span class="w"> </span>--serialized-file<span class="w"> </span>&lt;path_to_state_dict_file&gt;<span class="w"> </span><span class="o">[</span>--extra-files<span class="w"> </span>&lt;comma_seperarted_additional_files&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>--export-path<span class="w"> </span>&lt;output-dir&gt;<span class="w"> </span>--model-path<span class="w"> </span>&lt;model_dir&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>--runtime<span class="w"> </span>python3<span class="o">]</span>
</pre></div>
</div>
<p>NOTE -</p>
<ol class="simple">
<li><p>Options in [] are optional.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entry_point_function_name</span></code> can be skipped if it is named as <code class="docutils literal notranslate"><span class="pre">handle</span></code> in your <a class="reference external" href="#custom-handler-with-module-level-entry-point">handler module</a> or handler is <a class="reference external" href="#custom-handler-with-class-level-entry-point">python class</a></p></li>
</ol>
<p>This creates the file <code class="docutils literal notranslate"><span class="pre">&lt;model-name&gt;.mar</span></code> in the directory <code class="docutils literal notranslate"><span class="pre">&lt;output-dir&gt;</span></code> for python3 runtime. The <code class="docutils literal notranslate"><span class="pre">--runtime</span></code> parameter enables usage of a specific python version at runtime.
By default it uses the default python distribution of the system.</p>
<p>Example</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torch-model-archiver<span class="w"> </span>--model-name<span class="w"> </span>waveglow_synthesizer<span class="w"> </span>--version<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span>--model-file<span class="w"> </span>waveglow_model.py<span class="w"> </span>--serialized-file<span class="w"> </span>nvidia_waveglowpyt_fp32_20190306.pth<span class="w"> </span>--handler<span class="w"> </span>waveglow_handler.py<span class="w"> </span>--extra-files<span class="w"> </span>tacotron.zip,nvidia_tacotron2pyt_fp32_20190306.pth
</pre></div>
</div>
</section>
<section id="handling-model-execution-on-multiple-gpus">
<h2>Handling model execution on multiple GPUs<a class="headerlink" href="#handling-model-execution-on-multiple-gpus" title="Permalink to this heading">¶</a></h2>
<p>TorchServe scales backend workers on vCPUs or GPUs. In case of multiple GPUs TorchServe selects the gpu device in round-robin fashion and passes on this device id to the model handler in context object.
User should use this GPU ID for creating pytorch device object to ensure that all the workers are not created in the same GPU.
The following code snippet can be used in model handler to create the PyTorch device object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">ModelHandler</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A base Model handler implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">properties</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">system_properties</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gpu_id&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="installing-model-specific-python-dependencies">
<h2>Installing model specific python dependencies<a class="headerlink" href="#installing-model-specific-python-dependencies" title="Permalink to this heading">¶</a></h2>
<p>Custom models/handlers may depend on different python packages which are not installed by-default as a part of <code class="docutils literal notranslate"><span class="pre">TorchServe</span></code> setup.</p>
<p>Following steps allows user to supply a list of custom python packages to be installed by <code class="docutils literal notranslate"><span class="pre">TorchServe</span></code> for seamless model serving.</p>
<ol class="simple">
<li><p><a class="reference external" href="https://pytorch.org/serve/configuration.html#allow-model-specific-custom-python-packages">Enable model specific python package installation</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/serve/tree/master/model-archiver#torch-model-archiver-command-line-interface">Supply a requirements file with the model-archive</a>.</p></li>
</ol>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="default_handlers.html" class="btn btn-neutral float-right" title="TorchServe default inference handlers" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="configuration.html" class="btn btn-neutral" title="Advanced configuration" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, PyTorch Serve Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Custom Service</a><ul>
<li><a class="reference internal" href="#contents-of-this-document">Contents of this Document</a></li>
<li><a class="reference internal" href="#custom-handlers">Custom handlers</a><ul>
<li><a class="reference internal" href="#start-with-basehandler">Start with BaseHandler!</a></li>
<li><a class="reference internal" href="#custom-handler-with-module-level-entry-point">Custom handler with <code class="docutils literal notranslate"><span class="pre">module</span></code> level entry point</a></li>
<li><a class="reference internal" href="#custom-handler-with-class-level-entry-point">Custom handler with <code class="docutils literal notranslate"><span class="pre">class</span></code> level entry point</a></li>
<li><a class="reference internal" href="#advanced-custom-handlers">Advanced custom handlers</a><ul>
<li><a class="reference internal" href="#returning-custom-error-codes">Returning custom error codes</a></li>
<li><a class="reference internal" href="#writing-a-custom-handler-from-scratch-for-prediction-and-explanations-request">Writing a custom handler from scratch for Prediction and Explanations Request</a></li>
<li><a class="reference internal" href="#captum-explanations-for-custom-handler">Captum explanations for custom handler</a></li>
<li><a class="reference internal" href="#extend-default-handlers">Extend default handlers</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#creating-a-model-archive-with-an-entry-point">Creating a model archive with an entry point</a></li>
<li><a class="reference internal" href="#handling-model-execution-on-multiple-gpus">Handling model execution on multiple GPUs</a></li>
<li><a class="reference internal" href="#installing-model-specific-python-dependencies">Installing model specific python dependencies</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/katex.min.js"></script>
         <script src="_static/auto-render.min.js"></script>
         <script src="_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>