2022-11-05T00:04:14,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606654
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:04:14,143 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:04:14,143 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:04:21,037 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606661
2022-11-05T00:04:21,039 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:04:21,039 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:04:21,041 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:04:21,041 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:04:21,042 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:00,418 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606760
2022-11-05T00:06:00,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:00,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:00,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:00,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:01,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606761
2022-11-05T00:06:01,690 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:01,693 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:02,624 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606762
2022-11-05T00:06:02,625 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:04,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606764
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:07:55,084 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:07:55,085 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]26493
2022-11-05T00:07:55,085 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:07:55,086 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:07:55,100 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:07:55,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:07:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:07:55,281 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:08:14,625 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606894
2022-11-05T00:08:14,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:08:14,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:08:14,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:08:14,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:08:14,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:08:14,630 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:08:14,630 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:02,741 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:02,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:02,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:02,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 355, in raw_decode
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     raise JSONDecodeError("Expecting value", s, err.value) from None
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606942
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'')}]
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:02,747 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:02,747 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:02,747 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:02,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:02,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:32,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:32,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:32,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606972
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:1}')}]
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:32,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:32,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:32,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:32,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:44,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:44,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:44,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606984
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:1}')}]
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:44,940 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:44,940 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:44,940 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:55,274 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606995
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1}')}]
2022-11-05T00:09:55,278 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:55,278 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:55,279 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:55,279 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:55,279 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:05,354 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607005
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:')}]
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:05,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:05,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:05,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:05,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:05,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:05,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:13,396 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:13,396 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607013
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:')}]
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:13,400 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:13,400 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607022
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:')}]
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:22,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:22,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:41,561 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607041
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1')}]
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:41,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:41,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:41,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:41,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:41,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:12:52,307 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607172
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'1')}]
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:15:43,909 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:15:43,910 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28034
2022-11-05T00:15:43,910 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:15:43,910 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:15:43,925 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:15:43,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:15:44,100 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:15:44,108 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:16:07,875 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607367
2022-11-05T00:16:09,546 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:16:09,546 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254, in inference
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if isinstance(self.model, ort.InferenceSession):
2022-11-05T00:16:09,549 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'ort' is not defined
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607386
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254, in inference
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if isinstance(self.model, ort.InferenceSession):
2022-11-05T00:16:26,903 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'ort' is not defined
2022-11-05T00:19:50,138 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:50,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28643
2022-11-05T00:19:50,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:50,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:50,154 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:50,194 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:50,196 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:50,197 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:50,197 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:50,197 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:50,198 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:50,198 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:50,198 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:50,199 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:50,199 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:50,200 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:50,200 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:50,200 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:50,201 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:50,201 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:19:50,201 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:50,202 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:50,202 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:19:50,202 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:19:50,203 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:19:50,203 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:19:50,203 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:19:50,204 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:19:50,204 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:19:50,205 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:19:50,205 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:19:50,205 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:19:50,206 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:19:50,206 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:19:50,206 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:50,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:50,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:50,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:50,208 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:19:50,208 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:19:50,208 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254
2022-11-05T00:19:50,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if hasatrr(self.model, "run")
2022-11-05T00:19:50,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                 ^
2022-11-05T00:19:50,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax
2022-11-05T00:19:52,104 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:52,105 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28679
2022-11-05T00:19:52,106 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:52,106 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:52,108 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:52,130 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:52,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:52,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:52,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:52,134 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:52,134 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:52,134 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:52,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:52,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:52,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:52,137 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:19:54,017 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:54,018 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28694
2022-11-05T00:19:54,019 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:54,019 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:54,022 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:54,043 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:54,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:54,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:54,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:54,049 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:54,049 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:56,932 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:56,933 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28729
2022-11-05T00:19:56,933 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:56,933 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:56,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:56,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:56,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:56,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:56,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:56,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:56,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:56,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:56,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:56,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:00,868 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:00,868 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28759
2022-11-05T00:20:00,869 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:00,869 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:00,871 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:00,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:00,895 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:00,895 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:00,897 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:00,897 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:00,897 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:00,898 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:00,898 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:06,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:06,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28802
2022-11-05T00:20:06,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:06,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:06,787 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:06,808 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:06,811 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:06,811 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:06,811 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:15,695 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28838
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:15,698 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:15,720 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:15,722 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:15,722 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:29,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:29,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28990
2022-11-05T00:20:29,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:29,362 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:29,376 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:29,416 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:29,418 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:29,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:29,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:29,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:29,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:29,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:29,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:29,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:29,422 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:29,423 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:29,423 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:29,424 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:29,424 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:29,424 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:29,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:29,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:29,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:20:29,426 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:20:29,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:20:29,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:20:29,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:20:29,428 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:20:29,428 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:20:29,428 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:20:29,430 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:29,430 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:29,430 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:29,431 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:29,431 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:20:29,431 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:20:29,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:20:29,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:20:29,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if hasatrr(self.model, "run")
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                 ^
2022-11-05T00:20:29,434 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax
2022-11-05T00:20:31,331 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29026
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:31,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:31,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:31,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:31,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:31,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:31,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:31,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:31,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:31,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:31,362 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:33,235 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:33,236 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29056
2022-11-05T00:20:33,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:33,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:33,239 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:33,260 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:33,262 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:33,263 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:33,263 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:33,263 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:33,265 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:33,265 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:33,265 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:33,266 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:36,145 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29076
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:36,148 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:36,170 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:36,172 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:36,172 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:36,174 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:36,174 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:36,174 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:40,059 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:40,060 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29111
2022-11-05T00:20:40,061 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:40,061 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:40,063 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:40,085 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:40,090 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:40,090 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:40,090 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:57,909 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:57,911 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29286
2022-11-05T00:20:57,911 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:57,911 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:57,925 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:57,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:58,103 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:20:58,108 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:21:26,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607686
2022-11-05T00:21:28,413 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:21:28,413 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:21:28,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:21:28,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:21:28,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254, in inference
2022-11-05T00:21:28,416 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if hasatrr(self.model, "run"):
2022-11-05T00:21:28,416 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'hasatrr' is not defined
2022-11-05T00:22:03,236 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:22:03,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29620
2022-11-05T00:22:03,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:22:03,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:22:03,252 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:22:03,292 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:22:03,422 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:22:03,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:22:07,855 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607727
2022-11-05T00:22:09,462 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:22:09,464 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:22:09,464 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:22:09,464 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:22:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 255, in inference
2022-11-05T00:22:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.numpy().astype(np.float32)
2022-11-05T00:22:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
2022-11-05T00:23:02,308 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607782
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 255, in inference
2022-11-05T00:23:02,312 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     with torch.no_grad():
2022-11-05T00:23:02,312 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
2022-11-05T00:24:02,025 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:24:02,026 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29973
2022-11-05T00:24:02,027 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:24:02,027 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:24:02,041 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:24:02,082 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:24:02,216 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:24:02,221 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:24:06,496 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607846
2022-11-05T00:24:08,115 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:24:08,115 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 316, in handle
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 257, in inference
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.cpu().numpy().astype(np.float32)
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'np' is not defined
2022-11-05T00:24:54,701 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:24:54,702 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]30249
2022-11-05T00:24:54,702 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:24:54,703 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:24:54,717 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:24:54,757 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:24:54,888 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:24:54,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:25:05,639 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607905
2022-11-05T00:25:07,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 316, in handle
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 257, in inference
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.cpu().numpy().astype(np.float32)
2022-11-05T00:25:07,278 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'np' is not defined
2022-11-05T00:25:56,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:25:56,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]30535
2022-11-05T00:25:56,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:25:56,539 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:25:56,553 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:25:56,593 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:25:56,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:25:56,727 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:26:09,917 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607969
2022-11-05T00:26:11,508 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:26:11,510 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:26:11,510 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:26:11,510 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:26:11,511 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:26:11,511 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:26:11,511 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:26:11,512 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:26:11,512 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int64)) , expected: (tensor(float))
2022-11-05T00:27:11,841 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:27:11,841 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]30846
2022-11-05T00:27:11,842 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:27:11,842 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:27:11,856 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:27:11,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:27:12,027 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:27:12,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:27:15,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608035
2022-11-05T00:27:17,370 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:27:17,370 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:27:17,372 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:27:17,372 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:27:17,372 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:27:17,373 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:27:17,373 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:27:17,373 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:27:17,374 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))
2022-11-05T00:27:51,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:27:51,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31106
2022-11-05T00:27:51,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:27:51,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:27:51,491 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:27:51,532 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:27:51,663 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:27:51,668 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:27:54,437 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608074
2022-11-05T00:27:56,029 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:27:56,030 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:27:56,030 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:27:56,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 256, in inference
2022-11-05T00:27:56,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.float32().cpu().numpy()
2022-11-05T00:27:56,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - AttributeError: 'Tensor' object has no attribute 'float32'
2022-11-05T00:28:59,581 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:28:59,587 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31381
2022-11-05T00:28:59,587 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:28:59,587 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:28:59,641 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:28:59,769 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:28:59,912 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:28:59,917 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:29:06,103 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608146
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:29:07,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:29:07,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:29:07,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:29:07,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:29:07,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:29:07,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:29:07,785 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:29:07,785 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:29:07,785 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 0 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:30:37,241 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:30:37,242 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31711
2022-11-05T00:30:37,243 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:30:37,243 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:30:37,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:30:37,298 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:30:37,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:30:37,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:30:44,731 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608244
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:30:46,335 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:30:46,335 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:30:46,335 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:30:46,337 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:30:46,337 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 1 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:31:12,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:31:12,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31989
2022-11-05T00:31:12,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:31:12,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:31:12,484 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:31:12,525 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:31:12,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:31:12,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:31:16,661 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608276
2022-11-05T00:31:19,486 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:31:19,486 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:31:19,487 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:31:19,487 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:31:19,487 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:31:19,488 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:31:19,488 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:31:19,488 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:31:19,489 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:31:19,489 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:31:19,494 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:31:19,494 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 1 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:32:35,892 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:35,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32302
2022-11-05T00:32:35,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:35,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:35,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:35,949 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:35,951 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:35,951 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:35,952 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:35,952 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:35,953 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:35,953 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:35,954 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:35,954 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:35,955 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:35,955 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:35,955 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:35,956 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:35,956 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:35,956 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:35,957 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:35,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:35,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:32:35,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:32:35,959 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:32:35,959 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:32:35,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:32:35,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:32:35,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:32:35,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:32:35,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:35,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:35,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:32:35,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:32:35,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:32:35,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 240
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor([1,1], [1,1]], device=self.device)
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                        ^
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
2022-11-05T00:32:37,876 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:37,877 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32338
2022-11-05T00:32:37,878 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:37,878 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:37,880 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:37,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:37,906 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:37,906 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:37,906 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:39,796 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:39,797 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32370
2022-11-05T00:32:39,797 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:39,798 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:39,800 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:39,822 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:39,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:39,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:39,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:39,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:39,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:32:39,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:32:39,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:32:39,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:32:39,831 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:32:42,714 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32423
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:42,717 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:42,739 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:42,741 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:42,741 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:46,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:46,630 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32476
2022-11-05T00:32:46,631 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:46,631 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:46,633 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:46,654 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:46,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:46,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:46,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:52,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:52,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32511
2022-11-05T00:32:52,539 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:52,539 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:52,541 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:52,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:52,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:52,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:52,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:52,568 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:01,448 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32549
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:33:01,451 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:33:01,473 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:33:01,475 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:33:15,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32600
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:33:15,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:33:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:33:37,268 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32660
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:33:37,271 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:33:37,292 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:33:37,295 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:33:37,295 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:37,296 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:33:37,296 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:33:37,296 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:11,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:11,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32860
2022-11-05T00:34:11,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:11,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:11,271 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:11,312 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:11,315 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:11,315 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:11,316 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:11,316 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:11,317 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:11,317 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:11,317 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:11,318 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:11,318 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:11,318 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:11,319 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:11,319 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:11,319 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:11,320 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:11,320 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:11,320 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:34:11,322 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:34:11,322 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:34:11,322 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:34:11,323 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:34:11,323 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:34:11,324 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:34:11,324 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:34:11,324 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:34:11,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:11,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:11,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:11,326 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:11,326 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:34:11,327 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:34:11,327 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:34:11,327 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 240
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor([1,1], [1,1]], device=self.device)
2022-11-05T00:34:11,329 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                        ^
2022-11-05T00:34:11,329 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
2022-11-05T00:34:13,217 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32911
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:13,220 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:13,242 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:13,244 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:13,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:13,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:13,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:13,246 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:13,246 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:13,246 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:13,247 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:13,247 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:15,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:15,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32926
2022-11-05T00:34:15,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:15,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:15,138 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:15,160 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:15,162 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:15,163 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:15,163 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:15,163 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:18,044 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:18,045 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32958
2022-11-05T00:34:18,045 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:18,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:18,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:18,070 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:18,072 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:18,072 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:18,074 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:18,074 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:18,074 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:21,982 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32976
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:21,985 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:22,007 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:34:27,914 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:27,914 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33029
2022-11-05T00:34:27,915 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:27,915 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:27,917 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:27,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:27,945 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:52,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:52,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33196
2022-11-05T00:34:52,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:52,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:52,845 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:52,885 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:53,020 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:34:53,023 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:34:57,935 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608497
2022-11-05T00:34:59,533 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:34:59,535 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:34:59,535 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:34:59,535 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:34:59,536 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:34:59,536 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:34:59,536 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:34:59,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:34:59,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: modelInput for the following indices
2022-11-05T00:34:59,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -  index: 1 Got: 2 Expected: 1
2022-11-05T00:34:59,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -  Please fix either the inputs or the model.
2022-11-05T00:35:58,688 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:35:58,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33461
2022-11-05T00:35:58,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:35:58,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:35:58,704 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:35:58,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:35:58,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:35:58,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:35:58,749 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:35:58,749 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:35:58,750 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:35:58,750 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:35:58,751 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:35:58,751 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:35:58,751 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:35:58,752 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:35:58,753 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:35:58,753 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:35:58,754 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:35:58,754 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:35:58,754 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:35:58,755 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:35:58,755 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:35:58,755 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:35:58,756 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:35:58,756 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:35:58,756 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:35:58,757 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:35:58,757 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:35:58,758 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:35:58,758 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:35:58,760 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:35:58,760 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:35:58,760 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:35:58,761 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:35:58,761 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:35:58,761 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 240
2022-11-05T00:35:58,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor([1,1]], device=self.device)
2022-11-05T00:35:58,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                 ^
2022-11-05T00:35:58,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
2022-11-05T00:36:00,655 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:00,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33509
2022-11-05T00:36:00,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:00,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:00,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:00,682 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:00,684 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:00,684 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:00,684 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:00,685 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:00,685 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:00,687 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:00,687 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:00,688 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:02,582 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:02,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33542
2022-11-05T00:36:02,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:02,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:02,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:02,608 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:02,610 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:02,610 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:02,611 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:02,611 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:02,611 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:02,612 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:02,612 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:02,612 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:02,614 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:02,614 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:05,498 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33567
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:05,502 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:05,525 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:05,527 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:05,527 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:36:09,440 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33617
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:09,443 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:09,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:09,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:09,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:09,470 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:09,470 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:15,351 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:15,352 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33658
2022-11-05T00:36:15,352 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:15,353 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:15,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:15,376 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:15,378 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:15,382 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:37:06,971 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:37:06,973 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33873
2022-11-05T00:37:06,973 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:37:06,973 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:37:06,988 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:37:07,030 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:37:07,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:37:07,170 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:37:11,582 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608631
2022-11-05T00:37:13,231 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:37:13,233 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:37:13,233 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:37:13,233 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:37:13,234 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:37:13,234 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:37:13,234 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:37:13,235 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:37:13,235 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 1 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:38:50,229 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:38:50,230 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]34226
2022-11-05T00:38:50,230 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:38:50,231 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:38:50,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:38:50,286 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:38:50,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:38:50,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:38:53,638 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608733
2022-11-05T00:38:55,252 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:38:55,254 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:38:55,254 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:38:55,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:38:55,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:38:55,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:38:55,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference([1,1])
2022-11-05T00:38:55,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 256, in inference
2022-11-05T00:38:55,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.to(torch.float32).cpu().numpy()
2022-11-05T00:38:55,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - AttributeError: 'list' object has no attribute 'to'
2022-11-05T00:40:18,308 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:40:18,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]34532
2022-11-05T00:40:18,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:40:18,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:40:18,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:40:18,365 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:40:18,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:40:18,505 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:40:21,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608821
2022-11-05T00:40:21,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:40:21,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 308, in handle
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 237, in preprocess
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:40:35,129 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608835
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 308, in handle
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:40:35,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 237, in preprocess
2022-11-05T00:40:35,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:40:35,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:42:04,929 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:42:04,930 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]34957
2022-11-05T00:42:04,930 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:42:04,930 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:42:04,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:42:04,985 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:42:05,121 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:42:05,127 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:42:08,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608928
2022-11-05T00:42:10,086 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:42:10,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:42:10,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:42:10,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 260, in inference
2022-11-05T00:42:10,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return results
2022-11-05T00:42:10,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - UnboundLocalError: local variable 'results' referenced before assignment
2022-11-05T00:42:52,656 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:42:52,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]35199
2022-11-05T00:42:52,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:42:52,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:42:52,672 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:42:52,713 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:42:52,849 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:42:52,854 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:43:02,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:43:02,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]35342
2022-11-05T00:43:02,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:43:02,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:43:02,981 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:43:03,021 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:43:03,154 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:43:03,158 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:43:05,096 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608985
