2022-11-05T00:00:36,727 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,735 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24122619628906|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,735 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64096069335938|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,736 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,736 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,736 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,736 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,737 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58718.9453125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,737 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1868.77734375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:00:36,737 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606436
2022-11-05T00:01:36,750 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,750 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24202346801758|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,751 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64016342163086|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,751 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,751 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,751 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,751 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,752 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58696.8671875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,752 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1890.87109375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:01:36,752 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606496
2022-11-05T00:02:36,737 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,737 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24199295043945|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,737 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64019393920898|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,738 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,738 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,738 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,738 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,739 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58687.8359375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,739 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1899.90234375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:02:36,739 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606556
2022-11-05T00:03:12,125 [INFO ] epollEventLoopGroup-3-4 ACCESS_LOG - /127.0.0.1:40362 "POST /v2/models/onnx/predict HTTP/1.1" 404 0
2022-11-05T00:03:12,127 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:03:22,251 [INFO ] epollEventLoopGroup-3-5 ACCESS_LOG - /127.0.0.1:38104 "POST /v2/models/onnx/prediction HTTP/1.1" 404 0
2022-11-05T00:03:22,252 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:03:30,201 [INFO ] epollEventLoopGroup-3-6 ACCESS_LOG - /127.0.0.1:34902 "POST /models/onnx/prediction HTTP/1.1" 404 0
2022-11-05T00:03:30,202 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:03:36,739 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,739 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24197387695312|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,740 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64021301269531|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,740 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,740 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,740 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,741 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,741 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58635.4453125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,741 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1952.2890625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:03:36,741 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606616
2022-11-05T00:04:14,138 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606654138
2022-11-05T00:04:14,138 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606654138
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:04:14,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606654
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:34512 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:04:14,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:04:14,142 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 103400, Inference time ns: 4465966
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:04:14,142 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 103400, Inference time ns: 4465966
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606654
2022-11-05T00:04:14,142 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:04:14,143 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:04:14,143 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:04:21,036 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606661036
2022-11-05T00:04:21,036 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606661036
2022-11-05T00:04:21,037 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606661
2022-11-05T00:04:21,039 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:04:21,039 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:04:21,039 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:04:21,039 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:45568 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:04:21,040 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 121604, Inference time ns: 4557099
2022-11-05T00:04:21,040 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:04:21,040 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 121604, Inference time ns: 4557099
2022-11-05T00:04:21,041 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606661
2022-11-05T00:04:21,041 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:04:21,041 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:04:21,042 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:04:36,746 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,747 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.2421646118164|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,747 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64002227783203|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,747 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,747 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,747 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,748 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,748 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58596.73828125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,748 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1991.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:04:36,749 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606676
2022-11-05T00:05:36,734 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:14.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,734 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24215698242188|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,734 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64002990722656|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,734 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,735 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,735 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,735 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,735 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58591.296875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,735 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1996.43359375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:05:36,736 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606736
2022-11-05T00:06:00,417 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606760417
2022-11-05T00:06:00,417 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606760417
2022-11-05T00:06:00,418 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606760
2022-11-05T00:06:00,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:00,419 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:00,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:00,419 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:51674 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:00,420 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 100895, Inference time ns: 3956624
2022-11-05T00:06:00,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:00,420 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 100895, Inference time ns: 3956624
2022-11-05T00:06:00,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:00,421 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606760
2022-11-05T00:06:00,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:01,688 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606761688
2022-11-05T00:06:01,688 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606761688
2022-11-05T00:06:01,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606761
2022-11-05T00:06:01,690 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:51688 "POST /predictions/onnx HTTP/1.1" 503 3
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:06:01,691 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:01,692 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 118314, Inference time ns: 3551707
2022-11-05T00:06:01,692 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 118314, Inference time ns: 3551707
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606761
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:01,692 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:01,693 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:02,623 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606762623
2022-11-05T00:06:02,623 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606762623
2022-11-05T00:06:02,624 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606762
2022-11-05T00:06:02,625 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:51692 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:06:02,626 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:02,627 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 131385, Inference time ns: 3907973
2022-11-05T00:06:02,627 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 131385, Inference time ns: 3907973
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606762
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:02,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:04,581 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606764581
2022-11-05T00:06:04,581 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606764581
2022-11-05T00:06:04,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606764
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 310, in handle
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:06:04,584 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:51698 "POST /predictions/onnx HTTP/1.1" 503 3
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 238, in preprocess
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606247
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:06:04,585 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 101217, Inference time ns: 3693520
2022-11-05T00:06:04,585 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 101217, Inference time ns: 3693520
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:06:04,585 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606764
2022-11-05T00:06:36,733 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,733 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24201202392578|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,733 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64017486572266|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,734 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,734 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,734 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,734 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,734 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58520.6796875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,734 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2067.05859375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:06:36,735 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.5|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606796
2022-11-05T00:07:36,739 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,739 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24198150634766|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,739 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64020538330078|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,739 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,740 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,740 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,740 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,740 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58532.8203125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,740 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2054.91796875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:36,740 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.5|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606856
2022-11-05T00:07:53,914 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:07:53,914 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:07:54,003 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:07:54,003 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:07:54,012 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:07:54,012 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:07:54,064 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:07:54,064 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:07:54,082 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:07:54,082 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:07:54,082 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:07:54,082 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:07:54,082 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:07:54,082 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:07:54,083 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:07:54,083 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:07:54,093 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:07:54,093 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:07:54,095 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:07:54,095 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:07:54,184 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:07:54,184 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:07:54,184 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:07:54,184 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:07:54,185 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:07:54,185 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:07:54,186 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:07:54,186 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:07:54,187 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:07:54,187 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:07:54,401 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:07:54,401 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:07:54,951 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,951 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24197006225586|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,951 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64021682739258|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,952 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,952 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,952 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,953 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,953 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58827.75390625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,953 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1759.984375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:54,954 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606874
2022-11-05T00:07:55,084 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:07:55,085 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]26493
2022-11-05T00:07:55,085 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:07:55,086 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:07:55,086 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:07:55,086 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:07:55,091 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:07:55,091 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:07:55,100 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:07:55,103 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606875103
2022-11-05T00:07:55,103 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606875103
2022-11-05T00:07:55,141 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:07:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:07:55,281 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:07:55,289 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 148
2022-11-05T00:07:55,289 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 148
2022-11-05T00:07:55,290 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:07:55,290 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:07:55,291 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1201|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606875
2022-11-05T00:07:55,291 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:40|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606875
2022-11-05T00:08:14,624 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606894624
2022-11-05T00:08:14,624 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606894624
2022-11-05T00:08:14,625 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606894
2022-11-05T00:08:14,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2022-11-05T00:08:14,627 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:08:14,628 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:08:14,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:08:14,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:08:14,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:08:14,630 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:08:14,630 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:08:14,634 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:38848 "POST /predictions/onnx HTTP/1.1" 503 21
2022-11-05T00:08:14,634 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:08:14,635 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 170988, Inference time ns: 11318666
2022-11-05T00:08:14,635 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 170988, Inference time ns: 11318666
2022-11-05T00:08:14,635 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:08:54,896 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,897 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24194717407227|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,897 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64023971557617|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,897 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,897 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,898 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,898 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,898 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58719.74609375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,898 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1867.9921875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:08:54,899 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606934
2022-11-05T00:09:02,736 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606942736
2022-11-05T00:09:02,736 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606942736
2022-11-05T00:09:02,741 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:02,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:02,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:02,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5
2022-11-05T00:09:02,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:54444 "POST /predictions/onnx HTTP/1.1" 503 8
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 355, in raw_decode
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:09:02,744 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 98902, Inference time ns: 8317342
2022-11-05T00:09:02,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     raise JSONDecodeError("Expecting value", s, err.value) from None
2022-11-05T00:09:02,744 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 98902, Inference time ns: 8317342
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606942
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606942
2022-11-05T00:09:02,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'')}]
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:02,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:02,747 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:02,747 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:02,747 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:02,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:02,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:32,822 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606972822
2022-11-05T00:09:32,822 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606972822
2022-11-05T00:09:32,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:32,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:32,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:32,825 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:45316 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:09:32,826 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:09:32,827 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 109562, Inference time ns: 4906341
2022-11-05T00:09:32,827 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 109562, Inference time ns: 4906341
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606972
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606972
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:1}')}]
2022-11-05T00:09:32,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:32,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:32,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:32,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:32,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:32,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:44,935 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606984935
2022-11-05T00:09:44,935 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606984935
2022-11-05T00:09:44,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:44,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:44,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:44,937 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:53550 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:09:44,938 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:09:44,939 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 105193, Inference time ns: 4248369
2022-11-05T00:09:44,939 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 105193, Inference time ns: 4248369
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606984
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606984
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:1}')}]
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:44,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:44,940 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:44,940 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:44,940 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:44,941 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:09:54,899 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,900 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24192428588867|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,900 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64026260375977|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,900 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,900 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,901 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,901 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,901 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58664.328125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,901 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1923.41015625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:54,901 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606994
2022-11-05T00:09:55,273 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606995273
2022-11-05T00:09:55,273 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667606995273
2022-11-05T00:09:55,274 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:09:55,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:09:55,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:37780 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667606995
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:09:55,277 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 115004, Inference time ns: 4522674
2022-11-05T00:09:55,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1}')}]
2022-11-05T00:09:55,277 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 115004, Inference time ns: 4522674
2022-11-05T00:09:55,278 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:09:55,278 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606995
2022-11-05T00:09:55,278 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:09:55,279 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:09:55,279 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:09:55,279 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:09:55,280 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:05,353 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607005353
2022-11-05T00:10:05,353 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607005353
2022-11-05T00:10:05,354 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:05,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:10:05,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:48756 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:10:05,357 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 103388, Inference time ns: 4524332
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607005
2022-11-05T00:10:05,357 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 103388, Inference time ns: 4524332
2022-11-05T00:10:05,357 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:')}]
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607005
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:05,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:05,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:05,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:05,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:05,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:05,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:05,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:13,395 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607013395
2022-11-05T00:10:13,395 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607013395
2022-11-05T00:10:13,396 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:13,396 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:13,397 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:13,398 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:44170 "POST /predictions/onnx HTTP/1.1" 503 3
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607013
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:')}]
2022-11-05T00:10:13,399 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 117417, Inference time ns: 4132277
2022-11-05T00:10:13,399 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 117417, Inference time ns: 4132277
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607013
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:13,399 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:13,400 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:13,400 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:13,401 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:22,960 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607022960
2022-11-05T00:10:22,960 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607022960
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:22,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:22,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607022
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1:')}]
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:22,964 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:47828 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:22,965 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 103372, Inference time ns: 4764685
2022-11-05T00:10:22,965 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 103372, Inference time ns: 4764685
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607022
2022-11-05T00:10:22,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:22,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:22,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:41,560 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607041560
2022-11-05T00:10:41,560 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607041560
2022-11-05T00:10:41,561 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Failed json decoding of input data. Forwarding encoded payload
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/protocol/otf_message_handler.py", line 319, in _retrieve_input_data
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     model_input["value"] = json.loads(value.decode("utf-8"))
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/__init__.py", line 357, in loads
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _default_decoder.decode(s)
2022-11-05T00:10:41,562 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 337, in decode
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/json/decoder.py", line 353, in raw_decode
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     obj, end = self.scan_once(s, idx)
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:10:41,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607041
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'{1')}]
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:48562 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:10:41,564 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 122138, Inference time ns: 4462368
2022-11-05T00:10:41,564 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 122138, Inference time ns: 4462368
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:10:41,564 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607041
2022-11-05T00:10:41,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:10:41,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:10:41,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:10:41,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:10:41,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:10:54,882 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,883 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24186706542969|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,883 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64031982421875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,883 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,883 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,883 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,884 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,884 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58578.22265625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,884 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2009.515625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:10:54,884 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607054
2022-11-05T00:11:54,888 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,888 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24185943603516|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64032745361328|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58577.69140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,889 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2010.046875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:11:54,890 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607114
2022-11-05T00:12:52,306 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607172306
2022-11-05T00:12:52,306 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607172306
2022-11-05T00:12:52,307 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607172
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{'body': bytearray(b'1')}]
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:51394 "POST /predictions/onnx HTTP/1.1" 503 3
2022-11-05T00:12:52,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667606894
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 311, in handle
2022-11-05T00:12:52,310 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 137125, Inference time ns: 3735625
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:12:52,310 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 137125, Inference time ns: 3735625
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 239, in preprocess
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607172
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:12:52,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:12:54,896 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,896 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24184036254883|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,896 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64034652709961|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,896 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,897 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,897 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,897 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,897 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58554.74609375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,897 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2032.9921875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:12:54,897 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.5|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607174
2022-11-05T00:13:54,894 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:28.6|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,894 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24182891845703|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,894 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.6403579711914|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58545.88671875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2041.8515625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:13:54,895 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.5|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607234
2022-11-05T00:14:54,903 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.2418212890625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64036560058594|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58543.21484375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,904 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2044.5234375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:14:54,905 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.5|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607294
2022-11-05T00:15:42,646 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:15:42,646 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:15:42,743 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:15:42,743 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:15:42,753 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:15:42,753 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:15:42,809 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:15:42,809 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:15:42,827 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:15:42,827 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:15:42,828 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:15:42,828 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:15:42,828 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:15:42,828 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:15:42,828 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:15:42,828 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:15:42,840 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:15:42,840 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:15:42,841 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:15:42,841 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:15:42,934 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:15:42,934 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:15:42,934 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:15:42,934 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:15:42,936 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:15:42,936 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:15:42,936 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:15:42,936 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:15:42,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:15:42,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:15:43,181 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:15:43,181 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:15:43,740 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,740 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24179077148438|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,741 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64039611816406|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,741 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,742 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,742 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,743 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,743 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58840.32421875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,743 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1747.4140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,744 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607343
2022-11-05T00:15:43,909 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:15:43,910 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28034
2022-11-05T00:15:43,910 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:15:43,910 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:15:43,911 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:15:43,911 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:15:43,916 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:15:43,916 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:15:43,925 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:15:43,929 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607343929
2022-11-05T00:15:43,929 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607343929
2022-11-05T00:15:43,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:15:44,100 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:15:44,108 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:15:44,116 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:15:44,116 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:15:44,116 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:15:44,116 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:15:44,116 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1281|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607344
2022-11-05T00:15:44,117 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607344
2022-11-05T00:16:07,874 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607367874
2022-11-05T00:16:07,874 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607367874
2022-11-05T00:16:07,875 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607367
2022-11-05T00:16:09,546 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:16:09,546 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1672
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1672
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:16:09,547 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254, in inference
2022-11-05T00:16:09,548 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if isinstance(self.model, ort.InferenceSession):
2022-11-05T00:16:09,549 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'ort' is not defined
2022-11-05T00:16:09,556 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:57856 "POST /predictions/onnx HTTP/1.1" 503 1693
2022-11-05T00:16:09,556 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607367
2022-11-05T00:16:09,557 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 172672, Inference time ns: 1683215976
2022-11-05T00:16:09,557 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 172672, Inference time ns: 1683215976
2022-11-05T00:16:09,557 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607369
2022-11-05T00:16:26,898 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607386898
2022-11-05T00:16:26,898 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607386898
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607386
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:16:26,900 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:35844 "POST /predictions/onnx HTTP/1.1" 503 3
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607367
2022-11-05T00:16:26,901 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:16:26,901 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 105224, Inference time ns: 3179595
2022-11-05T00:16:26,901 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 105224, Inference time ns: 3179595
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607386
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254, in inference
2022-11-05T00:16:26,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if isinstance(self.model, ort.InferenceSession):
2022-11-05T00:16:26,903 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'ort' is not defined
2022-11-05T00:16:43,663 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,663 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24176788330078|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,664 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64041900634766|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,664 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,664 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,664 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,665 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,665 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57613.22265625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,665 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2964.515625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:16:43,665 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607403
2022-11-05T00:17:43,659 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,659 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24176025390625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,660 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64042663574219|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,660 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,660 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,660 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,661 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,661 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57613.01953125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,661 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2964.71875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:17:43,661 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607463
2022-11-05T00:18:43,660 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,661 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24173736572266|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,661 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64044952392578|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,661 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,662 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,662 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,662 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,662 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57624.25390625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,663 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2953.484375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:18:43,663 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607523
2022-11-05T00:19:48,932 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:19:48,932 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:19:49,026 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:19:49,026 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:19:49,035 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:19:49,035 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:19:49,087 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:19:49,087 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:19:49,106 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:19:49,106 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:19:49,106 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:19:49,106 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:19:49,107 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:19:49,107 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:19:49,107 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:19:49,107 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:19:49,118 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:49,118 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:49,120 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:19:49,120 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:19:49,212 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:19:49,212 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:19:49,213 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:19:49,213 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:19:49,214 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:19:49,214 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:19:49,215 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:19:49,215 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:19:49,216 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:19:49,216 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:19:49,451 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:19:49,451 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:19:50,026 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,027 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2416877746582|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,028 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64049911499023|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,028 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,028 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,029 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,029 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,029 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58816.046875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,030 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1771.69140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,030 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607590
2022-11-05T00:19:50,138 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:50,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28643
2022-11-05T00:19:50,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:50,139 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:50,140 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:19:50,140 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:19:50,145 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:50,145 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:50,154 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:50,157 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607590157
2022-11-05T00:19:50,157 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607590157
2022-11-05T00:19:50,194 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:50,196 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:50,197 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:50,197 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:50,197 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:50,198 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:50,198 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:50,198 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:50,198 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:50,198 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:50,199 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:50,199 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:50,199 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:50,199 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:50,200 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:50,200 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:50,200 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:50,201 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:50,201 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:19:50,201 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:50,202 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:50,202 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:19:50,202 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:19:50,203 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:19:50,203 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:19:50,203 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:19:50,204 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:19:50,204 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:19:50,205 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:19:50,205 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:19:50,205 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:19:50,206 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:19:50,206 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:19:50,206 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:50,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:50,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:50,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:50,208 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:19:50,208 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:19:50,208 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:19:50,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254
2022-11-05T00:19:50,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if hasatrr(self.model, "run")
2022-11-05T00:19:50,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                 ^
2022-11-05T00:19:50,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax
2022-11-05T00:19:50,200 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:50,200 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:50,216 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:50,216 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:50,217 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:50,217 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:50,217 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:50,217 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:50,217 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:50,217 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:50,218 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:19:50,218 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:19:50,234 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:50,234 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:50,234 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:50,234 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:51,219 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:51,219 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:52,104 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:52,105 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28679
2022-11-05T00:19:52,106 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:19:52,106 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:52,106 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:19:52,106 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:52,106 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:52,106 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:52,108 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607592108
2022-11-05T00:19:52,108 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:52,108 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607592108
2022-11-05T00:19:52,130 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:52,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:52,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:52,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:52,133 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:52,134 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:52,133 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:52,134 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:52,134 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:52,134 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:52,134 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:52,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:52,135 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:52,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:52,135 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:52,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:52,135 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:52,135 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:52,136 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:52,136 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:52,136 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:52,136 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:52,136 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:52,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:52,136 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:52,137 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:19:52,137 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:19:52,137 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:19:52,137 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:52,137 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:52,153 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:52,153 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:53,137 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:53,137 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:54,017 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:54,018 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28694
2022-11-05T00:19:54,019 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:19:54,019 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:54,019 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:19:54,019 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:54,019 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:54,019 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:54,022 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607594022
2022-11-05T00:19:54,022 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:54,022 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607594022
2022-11-05T00:19:54,043 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:54,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:54,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:54,046 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:54,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:54,046 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:54,047 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:54,047 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:54,047 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:54,048 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:54,048 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:54,048 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:54,048 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:54,049 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:54,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:54,049 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:54,049 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:54,049 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:54,049 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:54,049 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:54,049 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:54,049 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:19:54,049 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:19:54,049 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:19:54,050 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:54,050 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:54,066 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:54,066 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:56,050 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:56,050 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:56,932 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:19:56,933 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28729
2022-11-05T00:19:56,933 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:19:56,933 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:19:56,933 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:19:56,933 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:19:56,934 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:56,934 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:19:56,936 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607596936
2022-11-05T00:19:56,936 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:19:56,936 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607596936
2022-11-05T00:19:56,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:19:56,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:19:56,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:19:56,961 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:56,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:19:56,961 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:19:56,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:19:56,961 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:56,961 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:19:56,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:19:56,962 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:56,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:19:56,962 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:19:56,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:19:56,962 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:56,962 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:19:56,963 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:56,963 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:19:56,963 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:19:56,963 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:19:56,963 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:56,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:19:56,963 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:19:56,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:19:56,964 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:19:56,964 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:19:56,964 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:56,964 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:19:56,979 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:56,979 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:19:59,964 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:19:59,964 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:00,868 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:00,868 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28759
2022-11-05T00:20:00,869 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:00,869 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:00,869 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:00,869 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:00,869 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:00,869 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:00,871 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607600871
2022-11-05T00:20:00,871 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:00,871 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607600871
2022-11-05T00:20:00,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:00,895 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:00,895 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:00,896 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:00,896 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:00,896 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:00,896 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:00,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:00,896 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:00,897 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:00,896 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:00,897 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:00,897 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:00,897 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:00,897 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:00,897 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:00,898 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:00,897 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:00,898 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:00,898 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:00,898 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:00,898 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:00,898 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:20:00,898 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:20:00,898 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:00,899 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:00,899 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:00,914 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:00,914 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:05,899 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:05,899 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:06,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:06,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28802
2022-11-05T00:20:06,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:06,784 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:06,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:06,784 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:06,785 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:06,785 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:06,787 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:06,787 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607606787
2022-11-05T00:20:06,787 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607606787
2022-11-05T00:20:06,808 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:06,811 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:06,811 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:06,811 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:06,811 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:06,811 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:06,812 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:06,812 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:06,812 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:06,812 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:06,812 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:06,813 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:06,813 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:06,813 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:06,813 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:06,813 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:06,813 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:06,813 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:06,813 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:06,813 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:06,814 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:20:06,814 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:06,814 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:20:06,814 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:06,829 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:06,829 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:14,814 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:14,814 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:15,695 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28838
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:15,696 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:15,696 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:15,696 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:15,698 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607615698
2022-11-05T00:20:15,698 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:15,698 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607615698
2022-11-05T00:20:15,720 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:15,722 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:15,722 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:15,723 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:15,723 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:15,723 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:15,723 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:15,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:15,723 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:15,723 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:15,724 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:15,724 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:15,724 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:15,724 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:15,724 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:15,725 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:15,725 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:15,725 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:15,725 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:15,725 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:15,741 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:15,741 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:28,125 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:20:28,125 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:20:28,212 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:20:28,212 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:20:28,220 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:20:28,220 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:20:28,273 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:20:28,273 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:20:28,289 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:20:28,289 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:20:28,289 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:20:28,289 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:20:28,289 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:20:28,289 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:20:28,289 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:20:28,289 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:20:28,300 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:28,300 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:28,301 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:20:28,301 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:20:28,391 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:20:28,391 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:20:28,391 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:20:28,391 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:20:28,393 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:20:28,393 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:20:28,393 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:20:28,393 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:20:28,394 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:20:28,394 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:20:28,605 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:20:28,605 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:20:29,162 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:40.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,162 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24160385131836|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,163 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64058303833008|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,163 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,163 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,164 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,164 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,164 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58806.25390625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,165 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1781.484375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,165 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607629
2022-11-05T00:20:29,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:29,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]28990
2022-11-05T00:20:29,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:29,362 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:29,362 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:20:29,362 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:20:29,367 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:29,367 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:29,376 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:29,379 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607629379
2022-11-05T00:20:29,379 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607629379
2022-11-05T00:20:29,416 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:29,418 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:29,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:29,419 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:29,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:29,420 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:29,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:29,420 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:29,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:29,421 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:29,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:29,421 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:29,421 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:29,422 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:29,423 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:29,423 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:29,424 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:29,424 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:29,424 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:29,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:29,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:29,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:20:29,426 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:20:29,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:20:29,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:20:29,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:20:29,428 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:20:29,428 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:20:29,428 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:20:29,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:20:29,430 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:29,430 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:29,430 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:29,431 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:29,431 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:20:29,431 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:20:29,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:20:29,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:20:29,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if hasatrr(self.model, "run")
2022-11-05T00:20:29,433 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                 ^
2022-11-05T00:20:29,434 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax
2022-11-05T00:20:29,421 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:29,421 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:29,438 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:29,438 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:29,439 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:29,439 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:29,439 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:29,439 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:29,439 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:29,439 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:29,440 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:20:29,440 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:20:29,455 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:29,455 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:29,455 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:29,455 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:30,441 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:30,441 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:31,331 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29026
2022-11-05T00:20:31,332 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:31,332 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:31,332 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:31,334 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607631334
2022-11-05T00:20:31,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:31,334 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607631334
2022-11-05T00:20:31,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:31,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:31,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:31,359 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:31,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:31,359 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:31,359 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:31,360 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:31,360 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:31,360 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:31,360 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:31,360 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:31,361 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:31,361 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:31,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:31,361 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:31,361 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:31,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:31,361 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:31,361 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:31,361 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:31,362 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:31,362 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:31,362 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:31,362 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:20:31,363 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:31,362 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:20:31,363 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:31,382 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:31,382 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:32,363 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:32,363 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:33,235 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:33,236 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29056
2022-11-05T00:20:33,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:33,237 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:33,237 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:33,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:33,237 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:33,237 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:33,239 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:33,239 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607633239
2022-11-05T00:20:33,239 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607633239
2022-11-05T00:20:33,260 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:33,262 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:33,263 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:33,263 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:33,263 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:33,263 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:33,263 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:33,264 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:33,264 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:33,264 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:33,264 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:33,265 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:33,264 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:33,265 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:33,265 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:33,265 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:33,265 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:33,265 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:33,266 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:33,266 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:33,265 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:33,266 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:33,266 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:33,266 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:20:33,266 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:33,266 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:20:33,267 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:33,267 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:33,282 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:33,282 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:35,267 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:35,267 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:36,145 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29076
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:36,146 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:36,146 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:36,146 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:36,148 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:36,148 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607636148
2022-11-05T00:20:36,148 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607636148
2022-11-05T00:20:36,170 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:36,172 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:36,172 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:36,173 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:36,173 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:36,173 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:36,173 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:36,173 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:36,174 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:36,174 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:36,174 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:36,174 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:36,174 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:36,174 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:36,174 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:36,175 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:36,175 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:36,175 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:36,175 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:36,175 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:36,175 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:36,175 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:36,176 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:20:36,176 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:36,176 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:20:36,176 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:36,194 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:36,194 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:39,176 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:39,176 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:40,059 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:40,060 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29111
2022-11-05T00:20:40,061 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:40,061 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:40,061 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:20:40,061 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:40,061 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:40,061 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:40,063 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607640063
2022-11-05T00:20:40,063 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607640063
2022-11-05T00:20:40,063 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:40,085 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:20:40,088 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:40,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:20:40,088 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:20:40,089 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:20:40,089 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:20:40,089 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:40,089 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:20:40,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:20:40,090 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:40,090 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:20:40,090 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:20:40,090 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:40,090 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:20:40,090 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:20:40,090 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:40,090 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:20:40,090 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:20:40,091 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:20:40,091 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:40,091 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:20:40,107 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:40,107 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:20:45,091 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:45,091 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:56,713 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:20:56,713 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:20:56,803 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:20:56,803 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:20:56,812 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:20:56,812 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:20:56,870 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:20:56,870 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:20:56,887 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:20:56,887 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:20:56,887 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:20:56,887 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:20:56,887 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:20:56,887 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:20:56,887 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:20:56,887 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:20:56,898 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:56,898 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:20:56,899 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:20:56,899 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:20:56,994 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:20:56,994 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:20:56,994 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:20:56,994 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:20:56,996 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:20:56,996 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:20:56,996 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:20:56,996 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:20:56,997 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:20:56,997 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:20:57,208 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:20:57,208 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:20:57,749 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,749 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24151992797852|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,750 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64066696166992|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,750 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,750 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,751 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,751 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,751 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58790.6171875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,752 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1797.61328125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,752 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607657
2022-11-05T00:20:57,909 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:20:57,911 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29286
2022-11-05T00:20:57,911 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:20:57,911 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:20:57,912 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:20:57,912 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:20:57,917 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:57,917 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:20:57,925 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:20:57,928 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607657928
2022-11-05T00:20:57,928 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607657928
2022-11-05T00:20:57,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:20:58,103 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:20:58,108 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:20:58,115 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:20:58,115 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:20:58,116 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:20:58,116 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:20:58,116 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1222|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607658
2022-11-05T00:20:58,117 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607658
2022-11-05T00:21:26,823 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607686823
2022-11-05T00:21:26,823 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607686823
2022-11-05T00:21:26,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607686
2022-11-05T00:21:28,413 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:21:28,413 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1590
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1590
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:21:28,414 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:21:28,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:21:28,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:21:28,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 254, in inference
2022-11-05T00:21:28,416 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     if hasatrr(self.model, "run"):
2022-11-05T00:21:28,416 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'hasatrr' is not defined
2022-11-05T00:21:28,423 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:54848 "POST /predictions/onnx HTTP/1.1" 503 1611
2022-11-05T00:21:28,423 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607686
2022-11-05T00:21:28,424 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 173523, Inference time ns: 1600962944
2022-11-05T00:21:28,424 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 173523, Inference time ns: 1600962944
2022-11-05T00:21:28,424 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607688
2022-11-05T00:22:02,021 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:22:02,021 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:22:02,114 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:22:02,114 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:22:02,122 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:22:02,122 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:22:02,176 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:22:02,176 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:22:02,195 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:22:02,195 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:22:02,195 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:22:02,195 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:22:02,196 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:22:02,196 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:22:02,196 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:22:02,196 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:22:02,208 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:22:02,208 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:22:02,210 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:22:02,210 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:22:02,291 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:22:02,291 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:22:02,292 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:22:02,292 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:22:02,293 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:22:02,293 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:22:02,294 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:22:02,294 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:22:02,295 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:22:02,295 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:22:02,511 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:22:02,511 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:22:03,059 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,060 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2414779663086|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,061 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64070892333984|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,061 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,061 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,062 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,062 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,062 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58813.55078125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,063 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1774.1875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,063 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,236 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:22:03,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29620
2022-11-05T00:22:03,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:22:03,237 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:22:03,238 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:22:03,238 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:22:03,243 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:22:03,243 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:22:03,252 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:22:03,255 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607723255
2022-11-05T00:22:03,255 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607723255
2022-11-05T00:22:03,292 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:22:03,422 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:22:03,427 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:22:03,435 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:22:03,435 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:22:03,435 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:22:03,435 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:22:03,436 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1233|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:03,436 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607723
2022-11-05T00:22:07,854 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607727854
2022-11-05T00:22:07,854 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607727854
2022-11-05T00:22:07,855 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607727
2022-11-05T00:22:09,462 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1608
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1608
2022-11-05T00:22:09,463 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:22:09,464 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:22:09,464 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:22:09,464 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:22:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 255, in inference
2022-11-05T00:22:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.numpy().astype(np.float32)
2022-11-05T00:22:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
2022-11-05T00:22:09,472 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:54244 "POST /predictions/onnx HTTP/1.1" 503 1629
2022-11-05T00:22:09,472 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607727
2022-11-05T00:22:09,473 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 168271, Inference time ns: 1618842148
2022-11-05T00:22:09,473 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 168271, Inference time ns: 1618842148
2022-11-05T00:22:09,473 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607729
2022-11-05T00:23:02,307 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607782307
2022-11-05T00:23:02,307 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607782307
2022-11-05T00:23:02,308 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607782
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:23:02,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:33996 "POST /predictions/onnx HTTP/1.1" 503 5
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607727
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:23:02,311 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 98797, Inference time ns: 4403603
2022-11-05T00:23:02,311 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 98797, Inference time ns: 4403603
2022-11-05T00:23:02,311 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 255, in inference
2022-11-05T00:23:02,312 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,312 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     with torch.no_grad():
2022-11-05T00:23:02,312 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
2022-11-05T00:23:02,989 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,990 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24143981933594|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,990 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.6407470703125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,991 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,991 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,991 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,992 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,992 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57606.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,992 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2971.73828125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:23:02,993 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607782
2022-11-05T00:24:00,833 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:24:00,833 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:24:00,924 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:24:00,924 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:24:00,934 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:24:00,934 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:24:00,992 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:24:00,992 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:24:01,010 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:24:01,010 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:24:01,010 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:24:01,010 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:24:01,011 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:24:01,011 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:24:01,011 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:24:01,011 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:24:01,021 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:24:01,021 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:24:01,023 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:24:01,023 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:24:01,109 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:24:01,109 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:24:01,110 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:24:01,110 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:24:01,111 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:24:01,111 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:24:01,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:24:01,112 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:24:01,114 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:24:01,114 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:24:01,335 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:24:01,335 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:24:01,898 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,899 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24142456054688|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,900 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64076232910156|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,900 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,900 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,901 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,901 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,901 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58805.69140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,902 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1782.046875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:01,902 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607841
2022-11-05T00:24:02,025 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:24:02,026 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]29973
2022-11-05T00:24:02,027 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:24:02,027 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:24:02,027 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:24:02,027 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:24:02,033 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:24:02,033 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:24:02,041 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:24:02,045 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607842045
2022-11-05T00:24:02,045 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607842045
2022-11-05T00:24:02,082 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:24:02,216 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:24:02,221 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:24:02,230 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 148
2022-11-05T00:24:02,230 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 148
2022-11-05T00:24:02,231 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:24:02,231 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:24:02,231 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1214|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607842
2022-11-05T00:24:02,232 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607842
2022-11-05T00:24:06,494 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607846494
2022-11-05T00:24:06,494 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607846494
2022-11-05T00:24:06,496 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607846
2022-11-05T00:24:08,115 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:24:08,115 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1620
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1620
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:24:08,116 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 316, in handle
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 257, in inference
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.cpu().numpy().astype(np.float32)
2022-11-05T00:24:08,117 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'np' is not defined
2022-11-05T00:24:08,122 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:48816 "POST /predictions/onnx HTTP/1.1" 503 1639
2022-11-05T00:24:08,123 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607846
2022-11-05T00:24:08,123 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 180776, Inference time ns: 1628993281
2022-11-05T00:24:08,123 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 180776, Inference time ns: 1628993281
2022-11-05T00:24:08,123 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607848
2022-11-05T00:24:53,481 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:24:53,481 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:24:53,573 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:24:53,573 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:24:53,581 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:24:53,581 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:24:53,637 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:24:53,637 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:24:53,655 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:24:53,655 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:24:53,655 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:24:53,655 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:24:53,656 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:24:53,656 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:24:53,656 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:24:53,656 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:24:53,667 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:24:53,667 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:24:53,668 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:24:53,668 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:24:53,749 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:24:53,749 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:24:53,749 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:24:53,749 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:24:53,751 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:24:53,751 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:24:53,751 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:24:53,751 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:24:53,752 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:24:53,752 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:24:53,969 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:24:53,969 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:24:54,529 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,529 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24139785766602|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,530 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64078903198242|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,530 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,531 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,531 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,531 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,532 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58790.83984375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,532 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1796.8984375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,532 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,701 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:24:54,702 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]30249
2022-11-05T00:24:54,702 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:24:54,703 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:24:54,703 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:24:54,703 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:24:54,708 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:24:54,708 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:24:54,717 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:24:54,720 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607894720
2022-11-05T00:24:54,720 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607894720
2022-11-05T00:24:54,757 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:24:54,888 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:24:54,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:24:54,901 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:24:54,901 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:24:54,901 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:24:54,901 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:24:54,902 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1238|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:24:54,902 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607894
2022-11-05T00:25:05,638 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607905638
2022-11-05T00:25:05,638 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607905638
2022-11-05T00:25:05,639 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607905
2022-11-05T00:25:07,275 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1637
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1637
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:25:07,276 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 316, in handle
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 257, in inference
2022-11-05T00:25:07,277 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.cpu().numpy().astype(np.float32)
2022-11-05T00:25:07,278 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - NameError: name 'np' is not defined
2022-11-05T00:25:07,283 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:43974 "POST /predictions/onnx HTTP/1.1" 503 1656
2022-11-05T00:25:07,284 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607905
2022-11-05T00:25:07,284 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 167023, Inference time ns: 1646175009
2022-11-05T00:25:07,284 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 167023, Inference time ns: 1646175009
2022-11-05T00:25:07,284 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607907
2022-11-05T00:25:55,398 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:25:55,398 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:25:55,486 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:25:55,486 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:25:55,494 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:25:55,494 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:25:55,539 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:25:55,539 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:25:55,555 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:25:55,555 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:25:55,556 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:25:55,556 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:25:55,556 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:25:55,556 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:25:55,557 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:25:55,557 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:25:55,567 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:25:55,567 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:25:55,569 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:25:55,569 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:25:55,647 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:25:55,647 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:25:55,647 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:25:55,647 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:25:55,648 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:25:55,648 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:25:55,649 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:25:55,649 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:25:55,650 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:25:55,650 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:25:55,861 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:25:55,861 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:25:56,397 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,398 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24137496948242|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,398 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64081192016602|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,399 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,399 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,400 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,400 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58778.390625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,401 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1809.34765625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,401 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:25:56,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]30535
2022-11-05T00:25:56,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:25:56,539 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:25:56,539 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:25:56,539 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:25:56,544 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:25:56,544 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:25:56,553 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:25:56,556 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607956556
2022-11-05T00:25:56,556 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607956556
2022-11-05T00:25:56,593 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:25:56,723 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:25:56,727 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:25:56,736 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:25:56,736 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:25:56,736 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:25:56,736 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:25:56,737 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1174|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:25:56,737 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607956
2022-11-05T00:26:09,915 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607969915
2022-11-05T00:26:09,915 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667607969915
2022-11-05T00:26:09,917 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667607969
2022-11-05T00:26:11,508 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1593
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1593
2022-11-05T00:26:11,509 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:26:11,510 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:26:11,510 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:26:11,510 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:26:11,511 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:26:11,511 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:26:11,511 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:26:11,512 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:26:11,512 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int64)) , expected: (tensor(float))
2022-11-05T00:26:11,515 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:33118 "POST /predictions/onnx HTTP/1.1" 503 1610
2022-11-05T00:26:11,516 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607969
2022-11-05T00:26:11,516 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 164663, Inference time ns: 1601056875
2022-11-05T00:26:11,516 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 164663, Inference time ns: 1601056875
2022-11-05T00:26:11,517 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667607971
2022-11-05T00:26:56,337 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,338 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24134826660156|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,338 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64083862304688|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,338 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,339 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,339 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,339 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,339 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57634.328125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,339 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2943.41015625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:26:56,340 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608016
2022-11-05T00:27:10,656 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:27:10,656 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:27:10,744 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:27:10,744 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:27:10,753 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:27:10,753 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:27:10,800 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:27:10,800 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:27:10,819 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:27:10,819 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:27:10,819 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:27:10,819 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:27:10,820 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:27:10,820 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:27:10,820 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:27:10,820 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:27:10,831 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:27:10,831 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:27:10,832 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:27:10,832 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:27:10,914 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:27:10,914 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:27:10,914 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:27:10,914 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:27:10,916 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:27:10,916 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:27:10,916 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:27:10,916 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:27:10,917 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:27:10,917 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:27:11,128 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:27:11,128 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:27:11,667 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,668 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24133682250977|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,668 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64085006713867|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,668 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,669 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,669 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,669 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,670 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58817.10546875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,670 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1770.6328125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,670 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608031
2022-11-05T00:27:11,841 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:27:11,841 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]30846
2022-11-05T00:27:11,842 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:27:11,842 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:27:11,842 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:27:11,842 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:27:11,848 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:27:11,848 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:27:11,856 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:27:11,859 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608031859
2022-11-05T00:27:11,859 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608031859
2022-11-05T00:27:11,896 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:27:12,027 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:27:12,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:27:12,039 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:27:12,039 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:27:12,040 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:27:12,040 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:27:12,040 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1214|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608032
2022-11-05T00:27:12,041 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608032
2022-11-05T00:27:15,762 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608035762
2022-11-05T00:27:15,762 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608035762
2022-11-05T00:27:15,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608035
2022-11-05T00:27:17,370 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:27:17,370 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1608
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1608
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:27:17,371 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:27:17,372 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:27:17,372 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:27:17,372 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:27:17,373 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:27:17,373 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:27:17,373 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:27:17,374 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))
2022-11-05T00:27:17,378 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:34622 "POST /predictions/onnx HTTP/1.1" 503 1627
2022-11-05T00:27:17,378 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608035
2022-11-05T00:27:17,379 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 164308, Inference time ns: 1616877608
2022-11-05T00:27:17,379 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 164308, Inference time ns: 1616877608
2022-11-05T00:27:17,379 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608037
2022-11-05T00:27:50,297 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:27:50,297 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:27:50,386 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:27:50,386 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:27:50,394 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:27:50,394 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:27:50,442 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:27:50,442 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:27:50,459 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:27:50,459 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:27:50,460 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:27:50,460 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:27:50,460 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:27:50,460 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:27:50,460 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:27:50,460 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:27:50,471 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:27:50,471 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:27:50,472 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:27:50,472 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:27:50,546 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:27:50,546 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:27:50,547 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:27:50,547 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:27:50,548 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:27:50,548 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:27:50,548 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:27:50,548 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:27:50,550 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:27:50,550 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:27:50,765 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:27:50,765 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:27:51,326 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,327 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24131774902344|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,327 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.640869140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,327 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,328 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,328 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,329 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,329 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58796.49609375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,329 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1791.2421875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,330 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:27:51,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31106
2022-11-05T00:27:51,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:27:51,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:27:51,478 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:27:51,478 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:27:51,483 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:27:51,483 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:27:51,491 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:27:51,495 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608071495
2022-11-05T00:27:51,495 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608071495
2022-11-05T00:27:51,532 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:27:51,663 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:27:51,668 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:27:51,676 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 144
2022-11-05T00:27:51,676 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 144
2022-11-05T00:27:51,676 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:27:51,676 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:27:51,677 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1210|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:51,677 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:38|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608071
2022-11-05T00:27:54,435 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608074435
2022-11-05T00:27:54,435 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608074435
2022-11-05T00:27:54,437 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608074
2022-11-05T00:27:56,029 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:27:56,030 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:27:56,030 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1594
2022-11-05T00:27:56,030 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1594
2022-11-05T00:27:56,030 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:27:56,031 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:27:56,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 256, in inference
2022-11-05T00:27:56,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.float32().cpu().numpy()
2022-11-05T00:27:56,032 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - AttributeError: 'Tensor' object has no attribute 'float32'
2022-11-05T00:27:56,037 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:44088 "POST /predictions/onnx HTTP/1.1" 503 1612
2022-11-05T00:27:56,037 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608074
2022-11-05T00:27:56,037 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 171742, Inference time ns: 1602284015
2022-11-05T00:27:56,037 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 171742, Inference time ns: 1602284015
2022-11-05T00:27:56,038 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608076
2022-11-05T00:28:51,277 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,278 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24129867553711|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,278 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64088821411133|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,278 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,279 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,279 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,279 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,279 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57616.9296875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2960.80859375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:51,280 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608131
2022-11-05T00:28:57,351 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:28:57,351 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:28:57,445 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:28:57,445 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:28:57,456 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:28:57,456 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:28:57,512 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:28:57,512 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:28:57,529 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:28:57,529 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:28:57,529 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:28:57,529 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:28:57,529 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:28:57,529 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:28:57,530 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:28:57,530 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:28:57,541 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:28:57,541 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:28:57,543 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:28:57,543 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:28:57,629 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:28:57,629 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:28:57,629 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:28:57,629 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:28:57,631 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:28:57,631 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:28:57,632 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:28:57,632 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:28:57,633 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:28:57,633 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:28:57,862 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:28:57,862 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:28:58,987 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:37.5|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,988 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24128723144531|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,988 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64089965820312|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,993 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,993 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,994 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,994 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,994 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58796.125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,995 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1791.61328125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:58,995 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608138
2022-11-05T00:28:59,581 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:28:59,587 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31381
2022-11-05T00:28:59,587 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:28:59,587 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:28:59,588 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:28:59,588 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:28:59,606 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:28:59,606 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:28:59,641 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:28:59,653 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608139653
2022-11-05T00:28:59,653 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608139653
2022-11-05T00:28:59,769 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:28:59,912 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:28:59,917 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:28:59,925 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 155
2022-11-05T00:28:59,925 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 155
2022-11-05T00:28:59,926 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:28:59,926 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:28:59,926 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:2389|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608139
2022-11-05T00:28:59,927 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:119|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608139
2022-11-05T00:29:06,102 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608146102
2022-11-05T00:29:06,102 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608146102
2022-11-05T00:29:06,103 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608146
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1679
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:29:07,782 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1679
2022-11-05T00:29:07,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:29:07,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:29:07,783 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:29:07,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:29:07,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:29:07,784 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:29:07,785 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:29:07,785 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:29:07,785 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 0 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:29:07,790 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:57876 "POST /predictions/onnx HTTP/1.1" 503 1699
2022-11-05T00:29:07,790 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608146
2022-11-05T00:29:07,791 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 186983, Inference time ns: 1689070850
2022-11-05T00:29:07,791 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 186983, Inference time ns: 1689070850
2022-11-05T00:29:07,791 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608147
2022-11-05T00:29:58,350 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,350 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24127197265625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64091491699219|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,351 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,352 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57619.6953125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,352 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2953.0390625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:29:58,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608198
2022-11-05T00:30:36,063 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:30:36,063 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:30:36,154 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:30:36,154 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:30:36,163 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:30:36,163 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:30:36,216 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:30:36,216 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:30:36,240 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:30:36,240 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:30:36,241 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:30:36,241 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:30:36,241 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:30:36,241 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:30:36,241 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:30:36,241 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:30:36,254 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:30:36,254 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:30:36,256 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:30:36,256 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:30:36,336 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:30:36,336 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:30:36,336 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:30:36,336 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:30:36,337 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:30:36,337 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:30:36,338 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:30:36,338 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:30:36,339 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:30:36,339 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:30:36,586 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:30:36,586 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:30:37,125 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24125289916992|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64093399047852|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,127 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,127 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,127 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,128 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58800.62109375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,128 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1787.1171875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,128 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,241 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:30:37,242 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31711
2022-11-05T00:30:37,243 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:30:37,243 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:30:37,243 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:30:37,243 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:30:37,249 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:30:37,249 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:30:37,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:30:37,261 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608237261
2022-11-05T00:30:37,261 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608237261
2022-11-05T00:30:37,298 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:30:37,429 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:30:37,432 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:30:37,441 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:30:37,441 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 143
2022-11-05T00:30:37,441 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:30:37,441 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:30:37,442 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1192|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:37,443 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608237
2022-11-05T00:30:44,730 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608244730
2022-11-05T00:30:44,730 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608244730
2022-11-05T00:30:44,731 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608244
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1603
2022-11-05T00:30:46,334 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1603
2022-11-05T00:30:46,335 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:30:46,335 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:30:46,335 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:30:46,336 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:30:46,337 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:30:46,337 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 1 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:30:46,342 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:58060 "POST /predictions/onnx HTTP/1.1" 503 1623
2022-11-05T00:30:46,343 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608244
2022-11-05T00:30:46,343 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 168447, Inference time ns: 1613435210
2022-11-05T00:30:46,343 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 168447, Inference time ns: 1613435210
2022-11-05T00:30:46,344 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608246
2022-11-05T00:31:11,300 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:31:11,300 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:31:11,391 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:31:11,391 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:31:11,399 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:31:11,399 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:31:11,456 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:31:11,456 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:31:11,475 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:31:11,475 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:31:11,475 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:31:11,475 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:31:11,475 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:31:11,475 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:31:11,476 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:31:11,476 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:31:11,487 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:31:11,487 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:31:11,489 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:31:11,489 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:31:11,575 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:31:11,575 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:31:11,575 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:31:11,575 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:31:11,576 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:31:11,576 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:31:11,577 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:31:11,577 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:31:11,578 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:31:11,578 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:31:11,795 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:31:11,795 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:31:12,350 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2412338256836|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,351 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64095306396484|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,352 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,352 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,353 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58783.8828125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,353 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1803.85546875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,354 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:31:12,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]31989
2022-11-05T00:31:12,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:31:12,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:31:12,469 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:31:12,469 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:31:12,475 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:31:12,475 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:31:12,484 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:31:12,487 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608272487
2022-11-05T00:31:12,487 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608272487
2022-11-05T00:31:12,525 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:31:12,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:31:12,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:31:12,669 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 144
2022-11-05T00:31:12,669 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 144
2022-11-05T00:31:12,670 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:31:12,670 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:31:12,670 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1188|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:12,671 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:40|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608272
2022-11-05T00:31:16,656 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608276656
2022-11-05T00:31:16,656 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608276656
2022-11-05T00:31:16,661 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608276
2022-11-05T00:31:19,486 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:31:19,486 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:31:19,486 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2825
2022-11-05T00:31:19,486 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2825
2022-11-05T00:31:19,487 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:31:19,487 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:31:19,487 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:31:19,488 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:31:19,488 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:31:19,488 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:31:19,489 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:31:19,489 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:31:19,494 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:31:19,494 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 1 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:31:19,498 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:56494 "POST /predictions/onnx HTTP/1.1" 503 2879
2022-11-05T00:31:19,498 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608276
2022-11-05T00:31:19,499 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 234589, Inference time ns: 2843062131
2022-11-05T00:31:19,499 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 234589, Inference time ns: 2843062131
2022-11-05T00:31:19,499 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:18|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608279
2022-11-05T00:32:12,273 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,273 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24121475219727|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,273 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64097213745117|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,274 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,274 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,274 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,275 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,275 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57618.35546875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,275 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2954.37890625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:12,276 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608332
2022-11-05T00:32:34,670 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:32:34,670 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:32:34,760 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:32:34,760 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:32:34,768 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:32:34,768 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:32:34,823 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:32:34,823 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:32:34,840 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:32:34,840 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:32:34,840 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:32:34,840 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:32:34,840 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:32:34,840 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:32:34,841 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:32:34,841 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:32:34,851 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:34,851 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:34,853 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:32:34,853 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:32:34,933 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:32:34,933 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:32:34,934 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:32:34,934 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:32:34,936 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:32:34,936 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:32:34,936 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:32:34,936 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:32:34,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:32:34,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:32:35,148 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:32:35,148 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:32:35,711 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,711 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2411994934082|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,712 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64098739624023|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,712 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,713 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,713 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,713 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,714 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58811.49609375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,714 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1776.2421875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,715 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608355
2022-11-05T00:32:35,892 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:35,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32302
2022-11-05T00:32:35,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:35,893 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:35,894 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:32:35,894 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:32:35,899 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:35,899 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:35,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:35,911 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608355911
2022-11-05T00:32:35,911 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608355911
2022-11-05T00:32:35,949 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:35,951 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:35,951 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:35,952 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:35,952 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:35,953 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:35,953 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:35,953 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:35,953 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:35,954 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:35,954 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:35,954 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:35,954 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:35,955 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:35,955 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:35,955 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:35,956 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:35,956 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:35,956 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:35,957 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:35,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:35,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:32:35,958 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:32:35,959 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:32:35,959 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:32:35,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:32:35,960 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:32:35,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:32:35,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:32:35,961 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:32:35,962 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:35,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:35,963 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:32:35,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:32:35,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:32:35,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:32:35,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 240
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor([1,1], [1,1]], device=self.device)
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                        ^
2022-11-05T00:32:35,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
2022-11-05T00:32:35,954 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:35,954 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:35,971 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:35,971 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:35,971 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:35,971 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:35,972 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:35,972 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:35,972 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:35,972 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:35,973 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:32:35,973 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:32:35,991 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:35,991 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:35,991 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:35,991 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:36,973 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:36,973 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:37,876 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:37,877 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32338
2022-11-05T00:32:37,878 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:37,878 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:37,878 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:37,878 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:37,878 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:37,878 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:37,880 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:37,880 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608357880
2022-11-05T00:32:37,880 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608357880
2022-11-05T00:32:37,902 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:37,905 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:37,905 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:37,905 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:37,906 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:37,906 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:37,906 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:37,906 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:37,906 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:37,906 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:37,906 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:37,907 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:37,907 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:37,907 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:37,907 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:37,907 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:37,908 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:37,908 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:37,908 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:37,908 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:32:37,908 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:32:37,909 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:37,909 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:37,924 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:37,924 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:38,909 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:38,909 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:39,796 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:39,797 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32370
2022-11-05T00:32:39,797 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:39,798 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:39,798 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:39,798 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:39,798 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:39,798 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:39,800 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608359800
2022-11-05T00:32:39,800 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:39,800 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608359800
2022-11-05T00:32:39,822 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:39,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:39,824 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:39,825 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:39,825 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:39,826 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:39,825 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:39,826 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:39,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:39,826 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:39,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:39,826 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:39,826 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:39,827 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:32:39,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:32:39,826 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:39,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:32:39,826 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:39,830 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:39,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:32:39,830 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:39,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:32:39,830 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:39,830 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:39,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:32:39,831 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:39,831 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:39,831 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:32:39,831 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:32:39,831 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:32:39,831 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:39,831 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:39,846 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:39,846 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:41,831 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:41,831 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:42,714 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32423
2022-11-05T00:32:42,715 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:42,715 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:42,715 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:42,717 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608362717
2022-11-05T00:32:42,717 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:42,717 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608362717
2022-11-05T00:32:42,739 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:42,741 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:42,741 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:42,742 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:42,742 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:42,742 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:42,742 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:42,742 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:42,743 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:42,743 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:42,743 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:42,743 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:42,743 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:42,744 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:42,744 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:42,744 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:42,744 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:42,744 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:42,744 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:32:42,744 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:32:42,745 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:42,745 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:42,760 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:42,760 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:45,745 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:45,745 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:46,629 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:46,630 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32476
2022-11-05T00:32:46,631 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:46,631 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:46,631 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:46,631 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:46,631 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:46,631 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:46,633 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608366633
2022-11-05T00:32:46,633 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:46,633 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608366633
2022-11-05T00:32:46,654 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:46,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:46,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:46,657 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:46,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:46,657 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:46,658 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:46,658 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:46,658 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:46,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:46,658 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:46,659 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:46,659 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:46,659 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:46,659 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:46,660 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:46,659 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:46,660 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:46,660 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:46,660 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:32:46,660 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:46,676 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:46,676 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:51,661 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:51,661 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:32:52,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:32:52,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32511
2022-11-05T00:32:52,539 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:32:52,539 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:52,539 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:32:52,539 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:32:52,539 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:52,539 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:32:52,541 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:32:52,541 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608372541
2022-11-05T00:32:52,541 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608372541
2022-11-05T00:32:52,563 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:32:52,565 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:32:52,565 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:32:52,565 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:32:52,566 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:52,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:32:52,566 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:32:52,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:32:52,566 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:52,566 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:32:52,566 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:32:52,567 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:52,567 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:32:52,567 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:52,567 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:32:52,567 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:52,567 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:32:52,567 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:32:52,568 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:52,568 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:32:52,568 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:32:52,568 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:32:52,568 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:52,568 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:32:52,568 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:32:52,584 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:32:52,584 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:33:00,568 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:33:00,568 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:33:01,448 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32549
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:33:01,449 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:33:01,449 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:33:01,449 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:33:01,451 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:33:01,451 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608381451
2022-11-05T00:33:01,451 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608381451
2022-11-05T00:33:01,473 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:33:01,475 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:33:01,476 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:33:01,476 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:33:01,476 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:33:01,476 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:33:01,476 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:33:01,477 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:33:01,477 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:33:01,477 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:33:01,478 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:01,478 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:33:01,478 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:33:01,479 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:33:01,480 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:33:01,480 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:33:01,480 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:33:01,480 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:33:01,480 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:33:01,480 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:33:01,480 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:33:01,481 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-11-05T00:33:01,481 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:33:01,481 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-11-05T00:33:01,481 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:33:01,497 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:33:01,497 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:33:14,481 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:33:14,481 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:33:15,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32600
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:33:15,356 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:33:15,356 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:33:15,356 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:33:15,358 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608395358
2022-11-05T00:33:15,358 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:33:15,358 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608395358
2022-11-05T00:33:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:33:15,383 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:33:15,383 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:33:15,383 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:33:15,384 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:33:15,384 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:33:15,384 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:33:15,384 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:33:15,384 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:33:15,385 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:33:15,385 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:33:15,385 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:33:15,385 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:33:15,385 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:33:15,385 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:33:15,385 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:33:15,385 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-11-05T00:33:15,385 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-11-05T00:33:15,386 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:33:15,386 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:33:15,401 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:33:15,401 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:33:35,635 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:16.7|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,636 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:123.24110412597656|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,636 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:67.64108276367188|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,636 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,636 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,636 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,637 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,637 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:58596.99609375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,637 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1990.734375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:35,637 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:4.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608415
2022-11-05T00:33:36,386 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:33:36,386 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:33:37,268 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32660
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:33:37,269 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:33:37,269 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:33:37,269 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:33:37,271 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:33:37,271 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608417271
2022-11-05T00:33:37,271 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608417271
2022-11-05T00:33:37,292 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:33:37,295 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:33:37,295 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:33:37,295 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:33:37,296 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:33:37,296 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:33:37,295 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:33:37,296 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:33:37,296 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:33:37,296 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:33:37,296 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:33:37,296 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:33:37,296 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:33:37,296 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:33:37,297 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:33:37,297 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:33:37,297 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:33:37,297 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:33:37,297 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:33:37,297 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:33:37,297 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:33:37,298 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-11-05T00:33:37,298 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:33:37,298 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-11-05T00:33:37,298 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:33:37,314 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:33:37,314 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:10,020 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:34:10,020 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:34:10,111 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:34:10,111 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:34:10,119 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:34:10,119 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:34:10,170 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:34:10,170 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:34:10,188 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:34:10,188 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:34:10,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:34:10,189 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:34:10,189 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:34:10,189 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:34:10,189 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:34:10,189 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:34:10,200 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:10,200 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:10,202 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:34:10,202 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:34:10,290 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:34:10,290 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:34:10,291 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:34:10,291 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:34:10,292 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:34:10,292 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:34:10,293 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:34:10,293 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:34:10,294 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:34:10,294 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:34:10,509 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:34:10,509 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:34:11,056 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:40.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,057 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2410774230957|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,058 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64110946655273|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,058 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,058 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,059 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,059 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,060 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58812.05859375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,060 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1775.6796875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,060 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608451
2022-11-05T00:34:11,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:11,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32860
2022-11-05T00:34:11,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:11,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:11,257 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:34:11,257 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:34:11,263 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:11,263 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:11,271 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:11,275 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608451275
2022-11-05T00:34:11,275 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608451275
2022-11-05T00:34:11,312 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:11,315 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:11,315 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:11,316 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:11,316 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:11,316 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:11,317 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:11,316 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:11,317 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:11,317 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:11,317 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:11,317 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:11,318 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:11,318 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:11,318 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:11,319 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:11,319 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:11,319 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:11,320 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:11,320 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:11,320 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:34:11,321 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:34:11,322 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:34:11,322 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:34:11,322 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:34:11,323 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:34:11,323 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:34:11,324 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:34:11,324 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:34:11,324 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:34:11,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:11,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:11,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:11,326 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:11,326 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:34:11,327 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:34:11,327 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:34:11,327 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 240
2022-11-05T00:34:11,328 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor([1,1], [1,1]], device=self.device)
2022-11-05T00:34:11,329 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                        ^
2022-11-05T00:34:11,329 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
2022-11-05T00:34:11,318 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:11,318 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:11,336 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:11,336 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:11,337 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:11,337 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:11,337 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:11,337 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:11,337 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:11,337 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:11,338 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:34:11,338 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:34:11,353 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:11,353 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:11,353 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:11,353 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:12,339 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:12,339 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:13,217 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32911
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:13,218 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:13,218 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:13,218 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:13,220 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608453220
2022-11-05T00:34:13,220 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:13,220 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608453220
2022-11-05T00:34:13,242 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:13,244 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:13,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:13,245 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:13,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:13,245 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:13,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:13,246 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:13,246 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:13,246 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:13,246 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:13,246 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:13,246 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:13,246 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:13,247 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:13,247 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:13,247 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:13,247 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:13,247 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:13,247 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:13,248 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:13,248 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:13,248 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:13,248 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:34:13,249 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:13,248 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:34:13,249 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:13,265 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:13,265 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:14,249 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:14,249 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:15,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:15,135 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32926
2022-11-05T00:34:15,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:15,136 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:15,136 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:15,136 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:15,136 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:15,136 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:15,138 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:15,138 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608455138
2022-11-05T00:34:15,138 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608455138
2022-11-05T00:34:15,160 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:15,162 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:15,163 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:15,163 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:15,163 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:15,163 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:15,163 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:15,163 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:15,163 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:15,164 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:15,164 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:15,164 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:15,164 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:15,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:15,165 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:15,165 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:15,165 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:15,165 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:15,165 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:15,165 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:15,166 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:34:15,165 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:15,166 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:34:15,166 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:15,166 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:15,182 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:15,182 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:17,166 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:17,166 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:18,044 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:18,045 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32958
2022-11-05T00:34:18,046 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:18,045 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:18,046 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:18,046 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:18,046 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:18,046 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:18,048 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608458048
2022-11-05T00:34:18,048 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:18,048 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608458048
2022-11-05T00:34:18,070 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:18,072 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:18,072 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:18,072 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:18,072 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:18,073 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:18,073 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:18,073 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:18,074 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:18,074 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:18,074 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:18,074 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:18,074 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:18,074 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:18,074 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:18,075 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:18,075 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:18,075 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:18,075 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:18,075 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:18,075 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:18,075 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:18,076 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:34:18,076 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:18,076 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:34:18,076 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:18,092 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:18,092 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:21,076 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:21,076 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:21,982 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]32976
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:21,983 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:21,983 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:21,983 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:21,985 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608461985
2022-11-05T00:34:21,985 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:21,985 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608461985
2022-11-05T00:34:22,007 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:22,010 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:22,010 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:22,010 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:22,011 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:22,011 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:22,011 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:22,011 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:22,011 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:22,012 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:22,012 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:22,012 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:22,012 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:22,012 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:22,013 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:22,013 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:22,013 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:22,013 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:34:22,013 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:22,029 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:22,029 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:27,014 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:27,014 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:27,914 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:27,914 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33029
2022-11-05T00:34:27,915 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:27,915 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:27,915 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:34:27,915 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:27,915 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:27,915 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:27,917 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:27,917 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608467917
2022-11-05T00:34:27,917 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608467917
2022-11-05T00:34:27,939 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:34:27,942 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:34:27,942 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:34:27,942 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:34:27,943 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:27,943 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:34:27,943 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:27,943 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:34:27,943 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:34:27,944 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:27,944 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:34:27,944 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:34:27,944 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:27,945 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:27,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:34:27,945 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:34:27,945 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:27,945 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:34:27,945 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:34:27,945 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:34:27,945 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:27,945 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:34:27,945 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:34:27,961 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:27,961 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:34:51,642 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:34:51,642 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:34:51,735 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:34:51,735 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:34:51,744 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:34:51,744 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:34:51,794 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:34:51,794 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:34:51,810 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:34:51,810 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:34:51,811 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:34:51,811 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:34:51,811 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:34:51,811 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:34:51,811 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:34:51,811 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:34:51,822 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:51,822 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:34:51,823 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:34:51,823 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:34:51,909 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:34:51,909 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:34:51,910 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:34:51,910 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:34:51,911 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:34:51,911 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:34:51,912 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:34:51,912 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:34:51,913 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:34:51,913 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:34:52,137 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:34:52,137 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:34:52,697 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,698 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24099731445312|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,698 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64118957519531|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,699 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,699 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,699 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,700 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,700 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58779.6328125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,700 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1808.10546875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,701 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608492
2022-11-05T00:34:52,828 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:34:52,829 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33196
2022-11-05T00:34:52,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:34:52,830 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:34:52,831 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:34:52,831 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:34:52,836 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:52,836 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:34:52,845 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:34:52,848 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608492848
2022-11-05T00:34:52,848 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608492848
2022-11-05T00:34:52,885 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:34:53,020 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:34:53,023 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:34:53,032 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 147
2022-11-05T00:34:53,032 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 147
2022-11-05T00:34:53,033 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:34:53,033 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:34:53,033 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1215|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608493
2022-11-05T00:34:53,034 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608493
2022-11-05T00:34:57,934 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608497934
2022-11-05T00:34:57,934 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608497934
2022-11-05T00:34:57,935 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608497
2022-11-05T00:34:59,533 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1599
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1599
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:34:59,534 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:34:59,535 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:34:59,535 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:34:59,535 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:34:59,536 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:34:59,536 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:34:59,536 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:34:59,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:34:59,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: modelInput for the following indices
2022-11-05T00:34:59,537 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -  index: 1 Got: 2 Expected: 1
2022-11-05T00:34:59,538 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -  Please fix either the inputs or the model.
2022-11-05T00:34:59,541 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:39956 "POST /predictions/onnx HTTP/1.1" 503 1618
2022-11-05T00:34:59,541 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608497
2022-11-05T00:34:59,542 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 162675, Inference time ns: 1608052037
2022-11-05T00:34:59,542 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 162675, Inference time ns: 1608052037
2022-11-05T00:34:59,542 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608499
2022-11-05T00:35:51,249 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-11-05T00:35:51,249 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2022-11-05T00:35:57,474 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:35:57,474 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:35:57,564 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:35:57,564 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:35:57,573 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:35:57,573 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:35:57,632 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:35:57,632 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:35:57,648 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:35:57,648 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:35:57,649 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:35:57,649 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:35:57,649 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:35:57,649 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:35:57,649 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:35:57,649 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:35:57,660 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:35:57,660 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:35:57,661 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:35:57,661 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:35:57,737 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:35:57,737 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:35:57,737 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:35:57,737 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:35:57,738 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:35:57,738 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:35:57,739 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:35:57,739 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:35:57,740 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:35:57,740 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:35:57,964 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:35:57,964 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:35:58,547 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,548 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24097442626953|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,549 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.6412124633789|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,549 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,550 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,550 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,550 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,551 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58783.58984375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,551 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1804.1484375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,552 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608558
2022-11-05T00:35:58,688 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:35:58,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33461
2022-11-05T00:35:58,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:35:58,689 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:35:58,690 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:35:58,690 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:35:58,695 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:35:58,695 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:35:58,704 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:35:58,707 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608558707
2022-11-05T00:35:58,707 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608558707
2022-11-05T00:35:58,746 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:35:58,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:35:58,748 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:35:58,749 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:35:58,749 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:35:58,750 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:35:58,750 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:35:58,750 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:35:58,751 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:35:58,750 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:35:58,751 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:35:58,751 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:35:58,751 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:35:58,751 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:35:58,752 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:35:58,753 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:35:58,753 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:35:58,754 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:35:58,754 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:35:58,754 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:35:58,755 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:35:58,755 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:35:58,755 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:35:58,756 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 181, in run_server
2022-11-05T00:35:58,756 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-11-05T00:35:58,756 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 139, in handle_connection
2022-11-05T00:35:58,757 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-11-05T00:35:58,757 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 104, in load_model
2022-11-05T00:35:58,758 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     service = model_loader.load(
2022-11-05T00:35:58,758 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 102, in load
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 167, in _load_default_handler
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2022-11-05T00:35:58,759 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:35:58,760 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:35:58,760 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:35:58,760 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:35:58,761 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-11-05T00:35:58,761 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-11-05T00:35:58,761 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 839, in exec_module
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 976, in get_code
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-11-05T00:35:58,762 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 240
2022-11-05T00:35:58,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor([1,1]], device=self.device)
2022-11-05T00:35:58,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -                                 ^
2022-11-05T00:35:58,763 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
2022-11-05T00:35:58,752 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:35:58,752 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:35:58,769 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:35:58,769 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:35:58,769 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:35:58,769 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:35:58,770 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:35:58,770 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:35:58,770 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:35:58,770 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:35:58,770 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:35:58,770 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:35:58,786 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:35:58,786 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:35:58,786 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:35:58,786 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:35:59,771 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:35:59,771 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:00,655 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:00,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33509
2022-11-05T00:36:00,657 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:00,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:00,657 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:00,658 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:00,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:00,658 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:00,660 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608560660
2022-11-05T00:36:00,660 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608560660
2022-11-05T00:36:00,660 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:00,682 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:00,684 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:00,684 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:00,684 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:00,684 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:00,684 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:00,685 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:00,685 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:00,685 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:00,685 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:00,685 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:00,685 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:00,686 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:00,686 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:00,687 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:00,687 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:00,686 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:00,687 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:00,687 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:00,687 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:00,687 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:00,687 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:00,687 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:00,688 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:00,688 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:36:00,688 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:00,688 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-11-05T00:36:00,688 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:00,704 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:00,704 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:01,688 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:01,688 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:02,582 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:02,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33542
2022-11-05T00:36:02,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:02,583 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:02,583 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:02,583 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:02,584 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:02,584 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:02,585 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608562585
2022-11-05T00:36:02,585 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:02,585 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608562585
2022-11-05T00:36:02,608 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:02,610 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:02,610 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:02,610 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:02,611 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:02,610 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:02,611 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:02,611 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:02,611 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:02,611 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:02,612 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:02,612 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:02,612 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:02,612 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:02,612 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:02,613 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:02,613 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:02,613 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:02,613 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:02,613 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:02,613 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:02,613 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:02,614 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:02,614 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:02,614 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:02,614 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:36:02,614 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:02,614 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-11-05T00:36:02,615 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:02,615 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:02,635 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:02,635 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:04,615 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:04,615 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:05,498 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33567
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:05,500 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:05,500 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:05,500 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:05,502 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608565502
2022-11-05T00:36:05,502 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:05,502 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608565502
2022-11-05T00:36:05,525 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:05,527 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:05,527 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:05,528 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:05,528 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:05,528 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:05,528 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:05,528 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:05,529 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:05,529 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:05,529 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:05,530 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:05,530 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:05,530 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:05,530 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:05,530 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:05,530 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:05,530 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_service_worker.py", line 210, in <module>
2022-11-05T00:36:05,531 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:05,531 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:05,531 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     worker.run_server()
2022-11-05T00:36:05,532 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:36:05,532 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:05,532 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-11-05T00:36:05,532 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:05,549 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:05,549 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:08,532 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:08,532 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:09,440 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33617
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:09,441 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:09,441 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:09,441 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:09,443 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:09,443 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608569443
2022-11-05T00:36:09,443 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608569443
2022-11-05T00:36:09,465 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:09,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:09,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:09,467 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:09,467 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:09,467 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:09,468 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:09,468 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:09,468 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:09,468 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:09,468 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:09,469 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:09,469 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:09,469 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:09,469 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:09,469 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:09,470 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:09,470 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:09,470 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:09,470 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:09,470 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:09,470 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:09,470 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:36:09,471 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:09,470 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-11-05T00:36:09,471 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:09,486 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:09,486 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:14,471 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:14,471 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:36:15,351 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:36:15,352 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33658
2022-11-05T00:36:15,352 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:36:15,352 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:15,352 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-11-05T00:36:15,353 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:36:15,353 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:15,353 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:36:15,355 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608575355
2022-11-05T00:36:15,355 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:36:15,355 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608575355
2022-11-05T00:36:15,376 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:36:15,378 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend worker process died.
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 100, in load
2022-11-05T00:36:15,379 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2022-11-05T00:36:15,379 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-11-05T00:36:15,379 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/model_loader.py", line 162, in _load_handler_file
2022-11-05T00:36:15,380 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2022-11-05T00:36:15,380 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2022-11-05T00:36:15,380 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-11-05T00:36:15,380 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2022-11-05T00:36:15,380 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-11-05T00:36:15,380 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:15,380 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: onnx, error: Worker died.
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
2022-11-05T00:36:15,381 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'base_handler'
2022-11-05T00:36:15,381 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2022-11-05T00:36:15,381 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:15,381 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stderr
2022-11-05T00:36:15,381 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -
2022-11-05T00:36:15,382 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:15,382 [WARN ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-onnx_1.0-stdout
2022-11-05T00:36:15,382 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:36:15,382 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:36:15,382 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:15,382 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-11-05T00:36:15,382 [INFO ] W-9000-onnx_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stdout
2022-11-05T00:36:15,397 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:36:15,397 [INFO ] W-9000-onnx_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-onnx_1.0-stderr
2022-11-05T00:37:05,814 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:37:05,814 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:37:05,904 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:37:05,904 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:37:05,913 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:37:05,913 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:37:05,962 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:37:05,962 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:37:05,979 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:37:05,979 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:37:05,979 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:37:05,979 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:37:05,979 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:37:05,979 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:37:05,980 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:37:05,980 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:37:05,990 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:37:05,990 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:37:05,992 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:37:05,992 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:37:06,074 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:37:06,074 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:37:06,074 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:37:06,074 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:37:06,076 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:37:06,076 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:37:06,076 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:37:06,076 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:37:06,077 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:37:06,077 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:37:06,295 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:37:06,295 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:37:06,849 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,850 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24088668823242|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,850 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64130020141602|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,850 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,851 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,851 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,852 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,852 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58776.78125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,852 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1810.95703125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,853 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608626
2022-11-05T00:37:06,971 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:37:06,973 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]33873
2022-11-05T00:37:06,973 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:37:06,973 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:37:06,974 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:37:06,974 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:37:06,979 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:37:06,979 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:37:06,988 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:37:06,992 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608626992
2022-11-05T00:37:06,992 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608626992
2022-11-05T00:37:07,030 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:37:07,164 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:37:07,170 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:37:07,179 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 149
2022-11-05T00:37:07,179 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 149
2022-11-05T00:37:07,179 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:37:07,179 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:37:07,180 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1194|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608627
2022-11-05T00:37:07,181 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:40|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608627
2022-11-05T00:37:11,581 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608631581
2022-11-05T00:37:11,581 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608631581
2022-11-05T00:37:11,582 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608631
2022-11-05T00:37:13,231 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1650
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1650
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:37:13,232 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:37:13,233 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:37:13,233 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:37:13,233 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:37:13,234 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 258, in inference
2022-11-05T00:37:13,234 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     outputs = self.model.run(None, {"modelInput": data})[0]
2022-11-05T00:37:13,234 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/opt/conda/envs/ort/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 200, in run
2022-11-05T00:37:13,235 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return self._sess.run(output_names, input_feed, run_options)
2022-11-05T00:37:13,235 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: modelInput Got: 1 Expected: 2 Please fix either the inputs or the model.
2022-11-05T00:37:13,239 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:39106 "POST /predictions/onnx HTTP/1.1" 503 1669
2022-11-05T00:37:13,239 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608631
2022-11-05T00:37:13,240 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 180711, Inference time ns: 1658965694
2022-11-05T00:37:13,240 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 180711, Inference time ns: 1658965694
2022-11-05T00:37:13,240 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608633
2022-11-05T00:38:06,787 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,787 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24085998535156|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,788 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64132690429688|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,788 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,788 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,789 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,789 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,789 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57599.22265625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,790 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2973.51171875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:06,790 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608686
2022-11-05T00:38:49,047 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:38:49,047 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:38:49,139 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:38:49,139 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:38:49,148 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:38:49,148 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:38:49,203 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:38:49,203 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:38:49,219 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:38:49,219 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:38:49,219 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:38:49,219 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:38:49,219 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:38:49,219 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:38:49,220 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:38:49,220 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:38:49,230 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:38:49,230 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:38:49,232 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:38:49,232 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:38:49,312 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:38:49,312 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:38:49,313 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:38:49,313 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:38:49,314 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:38:49,314 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:38:49,315 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:38:49,315 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:38:49,316 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:38:49,316 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:38:49,571 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:38:49,571 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:38:50,132 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,135 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24085235595703|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,135 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.6413345336914|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,136 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,136 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,136 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,137 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,137 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58797.7265625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,137 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1790.01171875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,138 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,229 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:38:50,230 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]34226
2022-11-05T00:38:50,230 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:38:50,231 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:38:50,231 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:38:50,231 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:38:50,237 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:38:50,237 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:38:50,245 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:38:50,248 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608730248
2022-11-05T00:38:50,248 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608730248
2022-11-05T00:38:50,286 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:38:50,420 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:38:50,425 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:38:50,434 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 148
2022-11-05T00:38:50,434 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 148
2022-11-05T00:38:50,435 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:38:50,435 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:38:50,435 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1209|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:50,436 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:40|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608730
2022-11-05T00:38:53,637 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608733637
2022-11-05T00:38:53,637 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608733637
2022-11-05T00:38:53,638 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608733
2022-11-05T00:38:55,252 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [{}]
2022-11-05T00:38:55,253 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1615
2022-11-05T00:38:55,253 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1615
2022-11-05T00:38:55,254 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:38:55,254 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:38:55,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:38:55,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:38:55,255 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 315, in handle
2022-11-05T00:38:55,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference([1,1])
2022-11-05T00:38:55,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 256, in inference
2022-11-05T00:38:55,256 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data = data.to(torch.float32).cpu().numpy()
2022-11-05T00:38:55,257 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - AttributeError: 'list' object has no attribute 'to'
2022-11-05T00:38:55,260 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:52194 "POST /predictions/onnx HTTP/1.1" 503 1634
2022-11-05T00:38:55,261 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608733
2022-11-05T00:38:55,261 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 167630, Inference time ns: 1624422784
2022-11-05T00:38:55,261 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 167630, Inference time ns: 1624422784
2022-11-05T00:38:55,262 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608735
2022-11-05T00:40:17,126 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:40:17,126 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:40:17,217 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:40:17,217 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:40:17,226 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:40:17,226 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:40:17,276 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:40:17,276 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:40:17,294 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:40:17,294 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:40:17,294 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:40:17,294 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:40:17,295 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:40:17,295 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:40:17,295 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:40:17,295 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:40:17,306 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:40:17,306 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:40:17,307 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:40:17,307 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:40:17,395 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:40:17,395 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:40:17,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:40:17,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:40:17,397 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:40:17,397 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:40:17,398 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:40:17,398 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:40:17,399 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:40:17,399 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:40:17,640 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:40:17,640 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:40:18,190 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,190 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2408218383789|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,191 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64136505126953|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,191 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,191 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,192 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,192 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,192 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58757.71484375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,193 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1830.0234375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,193 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,308 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:40:18,309 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]34532
2022-11-05T00:40:18,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:40:18,310 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:40:18,310 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:40:18,310 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:40:18,316 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:40:18,316 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:40:18,325 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:40:18,328 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608818328
2022-11-05T00:40:18,328 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608818328
2022-11-05T00:40:18,365 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:40:18,500 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:40:18,505 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:40:18,515 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:40:18,515 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:40:18,515 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:40:18,515 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:40:18,516 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1215|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:18,517 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608818
2022-11-05T00:40:21,206 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608821206
2022-11-05T00:40:21,206 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608821206
2022-11-05T00:40:21,207 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608821
2022-11-05T00:40:21,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:40:21,209 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:40:21,210 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 308, in handle
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 237, in preprocess
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:40:21,211 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:40:21,216 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:59354 "POST /predictions/onnx HTTP/1.1" 503 21
2022-11-05T00:40:21,216 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608821
2022-11-05T00:40:21,216 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 169645, Inference time ns: 10527889
2022-11-05T00:40:21,216 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 169645, Inference time ns: 10527889
2022-11-05T00:40:21,217 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608821
2022-11-05T00:40:35,128 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608835128
2022-11-05T00:40:35,128 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608835128
2022-11-05T00:40:35,129 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608835
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2
2022-11-05T00:40:35,131 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:49762 "POST /predictions/onnx HTTP/1.1" 503 4
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608821
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 308, in handle
2022-11-05T00:40:35,132 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 123918, Inference time ns: 4178568
2022-11-05T00:40:35,132 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 123918, Inference time ns: 4178568
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     data_preprocess = self.preprocess(data)
2022-11-05T00:40:35,132 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608835
2022-11-05T00:40:35,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 237, in preprocess
2022-11-05T00:40:35,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return torch.as_tensor(data, device=self.device)
2022-11-05T00:40:35,133 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - RuntimeError: Could not infer dtype of dict
2022-11-05T00:41:18,140 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:14.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,141 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24079895019531|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,141 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64138793945312|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,141 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,142 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.01220703125|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,142 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:2|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,142 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,143 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58642.18359375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,143 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1945.5546875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:41:18,143 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608878
2022-11-05T00:42:03,688 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:42:03,688 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:42:03,791 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:42:03,791 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:42:03,800 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:42:03,800 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:42:03,845 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:42:03,845 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:42:03,864 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:42:03,864 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:42:03,865 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:42:03,865 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:42:03,865 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:42:03,865 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:42:03,865 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:42:03,865 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:42:03,877 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:42:03,877 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:42:03,879 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:42:03,879 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:42:03,972 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:42:03,972 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:42:03,972 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:42:03,972 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:42:03,974 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:42:03,974 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:42:03,974 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:42:03,974 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:42:03,975 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:42:03,975 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:42:04,200 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:42:04,200 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:42:04,772 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:33.3|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,773 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24078369140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,773 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64140319824219|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,774 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,774 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,774 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,775 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,775 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58777.59765625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,775 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1810.140625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,776 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608924
2022-11-05T00:42:04,929 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:42:04,930 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]34957
2022-11-05T00:42:04,930 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:42:04,930 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:42:04,930 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:42:04,930 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:42:04,936 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:42:04,936 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:42:04,944 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:42:04,948 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608924948
2022-11-05T00:42:04,948 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608924948
2022-11-05T00:42:04,985 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:42:05,121 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:42:05,127 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:42:05,135 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:42:05,135 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 150
2022-11-05T00:42:05,136 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:42:05,136 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:42:05,136 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1264|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608925
2022-11-05T00:42:05,137 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608925
2022-11-05T00:42:08,414 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608928414
2022-11-05T00:42:08,414 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608928414
2022-11-05T00:42:08,415 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608928
2022-11-05T00:42:10,086 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Invoking custom service failed.
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1672
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/service.py", line 102, in predict
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1672
2022-11-05T00:42:10,087 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)
2022-11-05T00:42:10,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 314, in handle
2022-11-05T00:42:10,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     output = self.inference(data_preprocess)
2022-11-05T00:42:10,088 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -   File "/home/ubuntu/serve/ts/torch_handler/base_handler.py", line 260, in inference
2022-11-05T00:42:10,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG -     return results
2022-11-05T00:42:10,089 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - UnboundLocalError: local variable 'results' referenced before assignment
2022-11-05T00:42:10,094 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:54858 "POST /predictions/onnx HTTP/1.1" 503 1691
2022-11-05T00:42:10,095 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608928
2022-11-05T00:42:10,095 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 175127, Inference time ns: 1681306804
2022-11-05T00:42:10,095 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 175127, Inference time ns: 1681306804
2022-11-05T00:42:10,096 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608930
2022-11-05T00:42:51,501 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:42:51,501 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:42:51,592 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:42:51,592 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:42:51,600 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:42:51,600 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:42:51,652 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:42:51,652 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:42:51,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:42:51,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:42:51,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:42:51,669 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:42:51,670 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:42:51,670 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:42:51,670 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:42:51,670 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:42:51,681 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:42:51,681 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:42:51,682 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:42:51,682 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:42:51,761 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:42:51,761 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:42:51,762 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:42:51,762 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:42:51,763 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:42:51,763 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:42:51,764 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:42:51,764 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:42:51,765 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:42:51,765 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:42:51,984 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:42:51,984 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:42:52,533 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,534 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24076080322266|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,535 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64142608642578|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,535 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,535 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,536 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,536 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,536 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58769.14453125|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,537 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1818.59375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,537 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,656 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:42:52,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]35199
2022-11-05T00:42:52,657 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:42:52,658 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:42:52,658 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:42:52,658 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:42:52,663 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:42:52,663 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:42:52,672 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:42:52,675 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608972675
2022-11-05T00:42:52,675 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608972675
2022-11-05T00:42:52,713 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:42:52,849 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:42:52,854 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:42:52,862 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 149
2022-11-05T00:42:52,862 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 149
2022-11-05T00:42:52,863 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:42:52,863 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:42:52,863 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1187|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:42:52,864 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608972
2022-11-05T00:43:01,753 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:43:01,753 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-11-05T00:43:01,849 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:43:01,849 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.6.0
TS Home: /home/ubuntu/serve
Current directory: /home/ubuntu/serve/test/pytest
Temp directory: /tmp
Number of GPUs: 1
Number of CPUs: 8
Max heap size: 15322 M
Python executable: /opt/conda/envs/ort/bin/python3.8
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ubuntu/serve/test/pytest/model_store
Initial Models: onnx.mar
Log dir: /home/ubuntu/serve/test/pytest/logs
Metrics dir: /home/ubuntu/serve/test/pytest/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/ubuntu/serve/test/pytest/model_store
Model config: N/A
2022-11-05T00:43:01,858 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:43:01,858 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-11-05T00:43:01,908 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:43:01,908 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: onnx.mar
2022-11-05T00:43:01,925 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:43:01,925 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model onnx
2022-11-05T00:43:01,926 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:43:01,926 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model onnx
2022-11-05T00:43:01,926 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:43:01,926 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model onnx loaded.
2022-11-05T00:43:01,926 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:43:01,926 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: onnx, count: 1
2022-11-05T00:43:01,938 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:43:01,938 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/envs/ort/bin/python3.8, /home/ubuntu/serve/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-11-05T00:43:01,939 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:43:01,939 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-11-05T00:43:02,018 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:43:02,018 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-11-05T00:43:02,018 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:43:02,018 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-11-05T00:43:02,019 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:43:02,019 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-11-05T00:43:02,020 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:43:02,020 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-11-05T00:43:02,021 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:43:02,021 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-11-05T00:43:02,234 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:43:02,234 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-11-05T00:43:02,787 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,788 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.2407455444336|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,788 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64144134521484|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,788 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,789 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,789 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,790 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,790 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:58782.8984375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,790 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1804.83984375|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,790 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:4.1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608982
2022-11-05T00:43:02,964 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-11-05T00:43:02,965 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - [PID]35342
2022-11-05T00:43:02,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Torch worker started.
2022-11-05T00:43:02,966 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Python runtime: 3.8.13
2022-11-05T00:43:02,966 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:43:02,966 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change null -> WORKER_STARTED
2022-11-05T00:43:02,972 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:43:02,972 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-11-05T00:43:02,981 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-11-05T00:43:02,984 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608982984
2022-11-05T00:43:02,984 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608982984
2022-11-05T00:43:03,021 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - model_name: onnx, batchSize: 1
2022-11-05T00:43:03,154 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - ONNX enabled
2022-11-05T00:43:03,158 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Missing the index_to_name.json file. Inference output will not include class name.
2022-11-05T00:43:03,165 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 144
2022-11-05T00:43:03,165 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 144
2022-11-05T00:43:03,166 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:43:03,166 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-onnx_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-11-05T00:43:03,166 [INFO ] W-9000-onnx_1.0 TS_METRICS - W-9000-onnx_1.0.ms:1233|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608983
2022-11-05T00:43:03,167 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:39|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608983
2022-11-05T00:43:05,095 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608985095
2022-11-05T00:43:05,095 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1667608985095
2022-11-05T00:43:05,096 [INFO ] W-9000-onnx_1.0-stdout MODEL_LOG - Backend received inference at: 1667608985
2022-11-05T00:43:06,740 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1644
2022-11-05T00:43:06,740 [INFO ] W-9000-onnx_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1644
2022-11-05T00:43:06,741 [INFO ] W-9000-onnx_1.0 ACCESS_LOG - /127.0.0.1:50854 "POST /predictions/onnx HTTP/1.1" 200 1657
2022-11-05T00:43:06,742 [INFO ] W-9000-onnx_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608985
2022-11-05T00:43:06,743 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218192, Backend time ns: 1648115208
2022-11-05T00:43:06,743 [DEBUG] W-9000-onnx_1.0 org.pytorch.serve.job.Job - Waiting time ns: 218192, Backend time ns: 1648115208
2022-11-05T00:43:06,743 [INFO ] W-9000-onnx_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608986
2022-11-05T00:43:06,743 [INFO ] W-9000-onnx_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667608986
2022-11-05T00:43:06,740 [INFO ] W-9000-onnx_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:1642.86|#ModelName:onnx,Level:Model|#hostname:ip-172-31-17-70,requestID:a8a10e79-83b8-4702-8636-b295e3439e15,timestamp:1667608986
2022-11-05T00:43:06,745 [INFO ] W-9000-onnx_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:1642.94|#ModelName:onnx,Level:Model|#hostname:ip-172-31-17-70,requestID:a8a10e79-83b8-4702-8636-b295e3439e15,timestamp:1667608986
2022-11-05T00:44:02,725 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,725 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:123.24072647094727|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,725 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:67.64146041870117|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,725 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:35.4|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,726 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:4.754638671875|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,726 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:779|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,726 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,727 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:57614.63671875|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,727 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2963.1015625|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
2022-11-05T00:44:02,727 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:6.0|#Level:Host|#hostname:ip-172-31-17-70,timestamp:1667609042
